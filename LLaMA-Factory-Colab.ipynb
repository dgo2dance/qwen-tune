{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/LLaMA-Factory-Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RomxmJkjnqE",
        "outputId": "be8e0a98-5c3d-4265-ccd2-8df03f29ae4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 24 05:31:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-XmSvFj1gg"
      },
      "source": [
        "#STEP 0 確認有使用GPU\n",
        "##記事本必須在GPU模式下運行，可以執行上方的nvidia-smi查看GPU是否有GPU。若没有，點擊左上角的 編輯>筆記本設定，把硬體加速器改成GPU。\n",
        "##如記事本有出問題可在以下儲存庫回報\n",
        "> https://github.com/ADT109119/LLaMA-Factory-Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f2PYE5UE4cCc",
        "outputId": "d3ef9380-b580-4f21-8396-bf2aea0f5e98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 6726, done.\u001b[K\n",
            "remote: Counting objects: 100% (745/745), done.\u001b[K\n",
            "remote: Compressing objects: 100% (300/300), done.\u001b[K\n",
            "remote: Total 6726 (delta 485), reused 653 (delta 443), pack-reused 5981\u001b[K\n",
            "Receiving objects: 100% (6726/6726), 204.47 MiB | 17.21 MiB/s, done.\n",
            "Resolving deltas: 100% (4836/4836), done.\n",
            "Updating files: 100% (139/139), done.\n",
            "/content/LLaMA-Factory\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Collecting transformers>=4.36.2 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.14.3 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.7.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.7.6 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.7.10-py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting einops (from -r requirements.txt (line 9))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 10))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.42.1)\n",
            "Collecting rouge-chinese (from -r requirements.txt (line 13))\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.8.1)\n",
            "Collecting uvicorn (from -r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.27.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.10.13)\n",
            "Collecting fastapi (from -r requirements.txt (line 17))\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 18))\n",
            "  Downloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.2->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.14.3->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.14.3->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.7.6->-r requirements.txt (line 6))\n",
            "  Downloading tyro-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.2.2)\n",
            "Collecting ffmpy (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (9.4.0)\n",
            "Collecting pydub (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->-r requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 14)) (1.3.2)\n",
            "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->-r requirements.txt (line 17))\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->-r requirements.txt (line 18)) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (2.8.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.2->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.2->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.2->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.2->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.2.0)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (13.7.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6))\n",
            "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=4929f5366330da3032aa33cdad72b242dd5f1178f8990d363b38ff96ee3babb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, ffmpy, websockets, typing-extensions, shtab, semantic-version, rouge-chinese, python-multipart, orjson, h11, einops, docstring-parser, dill, aiofiles, uvicorn, starlette, multiprocess, httpcore, tyro, httpx, fastapi, accelerate, transformers, sse-starlette, gradio-client, datasets, trl, peft, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.26.1 aiofiles-23.2.1 datasets-2.16.1 dill-0.3.7 docstring-parser-0.15 einops-0.7.0 fastapi-0.109.0 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 multiprocess-0.70.15 orjson-3.9.12 peft-0.7.1 pydub-0.25.1 python-multipart-0.0.6 rouge-chinese-1.0.3 semantic-version-2.10.0 sentencepiece-0.1.99 shtab-1.6.5 sse-starlette-1.8.2 starlette-0.35.1 transformers-4.37.0 trl-0.7.10 typing-extensions-4.9.0 tyro-0.7.0 uvicorn-0.27.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 1 複製儲存庫並安裝必要的函式庫\n",
        "#@markdown #STEP 1\n",
        "#@markdown ##複製儲存庫並安裝必要的函式庫\n",
        "#@markdown ##Clone repository & Install requirements lib\n",
        "\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd ./LLaMA-Factory\n",
        "!pip install kaleido\n",
        "!pip install -r requirements.txt\n",
        "!pip install bitsandbytes>=0.39.0\n",
        "\n",
        "#用比較殘暴的方式開啟 Gradio 的分享連結\n",
        "train_web_py_file_path = \"./src/train_web.py\"\n",
        "try:\n",
        "  with open(train_web_py_file_path, 'r') as file:\n",
        "    file_content = file.read()\n",
        "  modified_content = file_content.replace(\"share=False\", \"share=True\")\n",
        "  with open(train_web_py_file_path, 'w') as file:\n",
        "    file.write(modified_content)\n",
        "except Exception as e:\n",
        "    print(f'ERROR: {str(e)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install cohere  openai tiktoken\n",
        "! pip install transformers_stream_generator\n",
        "! pip install transformers==4.36.2\n",
        "! git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B\n",
        "!git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat"
      ],
      "metadata": {
        "id": "dXDuirW959WW",
        "outputId": "28453ca4-b773-42f9-d9f1-e2bea0c78b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.44-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, tiktoken, openai, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed backoff-2.2.1 cohere-4.44 fastavro-1.9.3 importlib_metadata-6.11.0 openai-1.9.0 tiktoken-0.5.2\n",
            "Collecting transformers_stream_generator\n",
            "  Downloading transformers-stream-generator-0.0.4.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.37.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2023.11.17)\n",
            "Building wheels for collected packages: transformers_stream_generator\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.4-py3-none-any.whl size=12316 sha256=f8c7028de7b6a86fe55f63a1610f513a2e5d6bf499d79c9a0c5c41f50c390b8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/1d/3c/92d88493ed40c0d9be60a391eb76c9a56e9f9b7542cb789401\n",
            "Successfully built transformers_stream_generator\n",
            "Installing collected packages: transformers_stream_generator\n",
            "Successfully installed transformers_stream_generator-0.0.4\n",
            "Collecting transformers==4.36.2\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2023.11.17)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.0\n",
            "    Uninstalling transformers-4.37.0:\n",
            "      Successfully uninstalled transformers-4.37.0\n",
            "Successfully installed transformers-4.36.2\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'Qwen-1_8B-Chat'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 58 (delta 18), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (58/58), 2.58 MiB | 5.95 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.42 GiB | 49.51 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiFJX0-d6thC",
        "outputId": "5a49b24b-7025-43bd-e29f-490a694a6b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-01-24 05:36:01.494019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:36:01.494072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:36:01.495416: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:36:02.751093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://5c716d1f6706ab3eec.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "01/24/2024 05:37:18 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1838] 2024-01-24 05:37:18,425 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1751: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "01/24/2024 05:37:18 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "01/24/2024 05:37:18 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/runs/Jan24_05-37-18_0feddcc94c16,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "tokenizer_config.json: 100% 173/173 [00:00<00:00, 974kB/s]\n",
            "tokenization_qwen.py: 100% 9.62k/9.62k [00:00<00:00, 37.0MB/s]\n",
            "qwen.tiktoken: 100% 2.56M/2.56M [00:00<00:00, 6.26MB/s]\n",
            "[INFO|tokenization_utils_base.py:2026] 2024-01-24 05:37:19,931 >> loading file qwen.tiktoken from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/qwen.tiktoken\n",
            "[INFO|tokenization_utils_base.py:2026] 2024-01-24 05:37:19,932 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2026] 2024-01-24 05:37:19,932 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2026] 2024-01-24 05:37:19,932 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2024-01-24 05:37:19,932 >> loading file tokenizer.json from cache at None\n",
            "config.json: 100% 910/910 [00:00<00:00, 5.09MB/s]\n",
            "[INFO|configuration_utils.py:739] 2024-01-24 05:37:20,823 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "configuration_qwen.py: 100% 2.35k/2.35k [00:00<00:00, 15.3MB/s]\n",
            "[INFO|configuration_utils.py:739] 2024-01-24 05:37:21,132 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/config.json\n",
            "[INFO|configuration_utils.py:802] 2024-01-24 05:37:21,133 >> Model config QWenConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen-1_8B-Chat\",\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Qwen/Qwen-1_8B-Chat--configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"Qwen/Qwen-1_8B-Chat--modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "modeling_qwen.py: 100% 55.6k/55.6k [00:00<00:00, 689kB/s]\n",
            "cpp_kernels.py: 100% 1.92k/1.92k [00:00<00:00, 10.1MB/s]\n",
            "qwen_generation_utils.py: 100% 14.6k/14.6k [00:00<00:00, 50.6MB/s]\n",
            "model.safetensors.index.json: 100% 14.7k/14.7k [00:00<00:00, 49.2MB/s]\n",
            "[INFO|modeling_utils.py:3344] 2024-01-24 05:37:22,376 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/2.04G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 10.5M/2.04G [00:00<00:28, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 31.5M/2.04G [00:00<00:15, 131MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 62.9M/2.04G [00:00<00:10, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 105M/2.04G [00:00<00:07, 245MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 136M/2.04G [00:00<00:08, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 168M/2.04G [00:00<00:08, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 199M/2.04G [00:00<00:08, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 231M/2.04G [00:01<00:09, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 252M/2.04G [00:01<00:10, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 283M/2.04G [00:01<00:09, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 315M/2.04G [00:01<00:08, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 346M/2.04G [00:01<00:07, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 377M/2.04G [00:01<00:09, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 398M/2.04G [00:02<00:09, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 419M/2.04G [00:02<00:09, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 440M/2.04G [00:02<00:09, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 472M/2.04G [00:02<00:08, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 503M/2.04G [00:02<00:07, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 524M/2.04G [00:02<00:07, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 545M/2.04G [00:02<00:07, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 577M/2.04G [00:03<00:07, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 608M/2.04G [00:03<00:06, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 640M/2.04G [00:03<00:06, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 671M/2.04G [00:03<00:05, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 703M/2.04G [00:03<00:05, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 734M/2.04G [00:03<00:05, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 765M/2.04G [00:03<00:05, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 797M/2.04G [00:03<00:05, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 828M/2.04G [00:05<00:27, 44.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 849M/2.04G [00:06<00:23, 50.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 870M/2.04G [00:06<00:18, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 902M/2.04G [00:06<00:13, 82.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 933M/2.04G [00:06<00:10, 108MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 965M/2.04G [00:06<00:08, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 996M/2.04G [00:06<00:07, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.03G/2.04G [00:06<00:06, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 1.05G/2.04G [00:07<00:05, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 1.08G/2.04G [00:07<00:05, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 1.11G/2.04G [00:07<00:04, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 1.14G/2.04G [00:07<00:03, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 1.17G/2.04G [00:07<00:03, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 1.21G/2.04G [00:07<00:03, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 1.24G/2.04G [00:07<00:03, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 1.27G/2.04G [00:07<00:03, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 1.30G/2.04G [00:08<00:03, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 1.33G/2.04G [00:08<00:02, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 1.36G/2.04G [00:08<00:02, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 1.39G/2.04G [00:08<00:02, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 1.43G/2.04G [00:08<00:02, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 1.46G/2.04G [00:08<00:02, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 1.49G/2.04G [00:08<00:02, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 1.52G/2.04G [00:08<00:02, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 1.55G/2.04G [00:09<00:01, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 1.58G/2.04G [00:09<00:01, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 1.61G/2.04G [00:09<00:01, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 1.65G/2.04G [00:09<00:01, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 1.68G/2.04G [00:09<00:01, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 1.71G/2.04G [00:09<00:01, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 1.74G/2.04G [00:09<00:01, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 1.77G/2.04G [00:09<00:01, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 1.80G/2.04G [00:10<00:00, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 1.84G/2.04G [00:10<00:00, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 1.87G/2.04G [00:10<00:01, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 1.89G/2.04G [00:10<00:01, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 1.91G/2.04G [00:11<00:01, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 1.93G/2.04G [00:11<00:01, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 1.95G/2.04G [00:11<00:00, 95.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 1.97G/2.04G [00:11<00:00, 89.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 1.98G/2.04G [00:11<00:00, 89.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 2.00G/2.04G [00:12<00:00, 108MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 2.02G/2.04G [00:12<00:00, 88.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 2.04G/2.04G [00:12<00:00, 161MB/s] \n",
            "Downloading shards:  50% 1/2 [00:13<00:13, 13.01s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/1.63G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/1.63G [00:00<00:11, 139MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 41.9M/1.63G [00:00<00:10, 157MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 62.9M/1.63G [00:00<00:10, 157MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 83.9M/1.63G [00:00<00:09, 171MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 105M/1.63G [00:00<00:08, 181MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 126M/1.63G [00:00<00:08, 182MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 147M/1.63G [00:00<00:08, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 168M/1.63G [00:00<00:08, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 199M/1.63G [00:01<00:07, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 231M/1.63G [00:01<00:06, 215MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 262M/1.63G [00:01<00:06, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 294M/1.63G [00:01<00:05, 227MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 325M/1.63G [00:01<00:06, 196MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 346M/1.63G [00:01<00:06, 195MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 377M/1.63G [00:01<00:06, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 409M/1.63G [00:02<00:05, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 440M/1.63G [00:02<00:05, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 472M/1.63G [00:02<00:05, 217MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 503M/1.63G [00:02<00:06, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 524M/1.63G [00:02<00:06, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 545M/1.63G [00:02<00:05, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 566M/1.63G [00:02<00:05, 190MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 587M/1.63G [00:03<00:05, 188MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 608M/1.63G [00:03<00:05, 185MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 650M/1.63G [00:03<00:04, 207MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 682M/1.63G [00:03<00:04, 211MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 713M/1.63G [00:03<00:04, 226MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 744M/1.63G [00:03<00:03, 228MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 776M/1.63G [00:03<00:03, 222MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 807M/1.63G [00:04<00:03, 229MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 839M/1.63G [00:04<00:03, 223MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 870M/1.63G [00:04<00:03, 235MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 902M/1.63G [00:04<00:03, 237MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 933M/1.63G [00:04<00:02, 241MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 965M/1.63G [00:04<00:02, 240MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 996M/1.63G [00:04<00:02, 245MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.03G/1.63G [00:04<00:02, 242MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.06G/1.63G [00:05<00:02, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.09G/1.63G [00:05<00:02, 246MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.12G/1.63G [00:05<00:02, 250MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.15G/1.63G [00:05<00:01, 251MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.18G/1.63G [00:05<00:01, 254MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.22G/1.63G [00:05<00:01, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 1.25G/1.63G [00:05<00:01, 249MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 1.28G/1.63G [00:05<00:01, 247MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 1.31G/1.63G [00:06<00:01, 252MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 1.34G/1.63G [00:06<00:01, 249MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 1.37G/1.63G [00:06<00:01, 258MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 1.41G/1.63G [00:06<00:00, 251MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 1.44G/1.63G [00:06<00:00, 249MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 1.47G/1.63G [00:06<00:00, 245MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 1.50G/1.63G [00:06<00:00, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 1.53G/1.63G [00:06<00:00, 244MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 1.56G/1.63G [00:07<00:00, 253MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 1.59G/1.63G [00:07<00:00, 247MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 1.63G/1.63G [00:07<00:00, 222MB/s]\n",
            "Downloading shards: 100% 2/2 [00:20<00:00, 10.30s/it]\n",
            "[INFO|modeling_utils.py:1341] 2024-01-24 05:37:42,982 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:826] 2024-01-24 05:37:42,984 >> Generate config GenerationConfig {}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.95s/it]\n",
            "[INFO|modeling_utils.py:4185] 2024-01-24 05:37:47,355 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4193] 2024-01-24 05:37:47,355 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-1_8B-Chat.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
            "generation_config.json: 100% 249/249 [00:00<00:00, 1.32MB/s]\n",
            "[INFO|configuration_utils.py:781] 2024-01-24 05:37:47,584 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2024-01-24 05:37:47,585 >> Generate config GenerationConfig {\n",
            "  \"chat_format\": \"chatml\",\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 512,\n",
            "  \"max_window_size\": 6144,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"top_k\": 0,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[WARNING|modeling_utils.py:2045] 2024-01-24 05:37:47,586 >> You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "01/24/2024 05:37:47 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "01/24/2024 05:37:47 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "01/24/2024 05:37:50 - INFO - llmtuner.model.loader - trainable params: 1572864 || all params: 1838401536 || trainable%: 0.0856\n",
            "01/24/2024 05:37:50 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
            "01/24/2024 05:37:50 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "01/24/2024 05:37:50 - INFO - llmtuner.data.template - Replace eos token: <|im_end|>\n",
            "01/24/2024 05:37:50 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n",
            "Using custom data configuration default-8c4d76b2be978785\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-8c4d76b2be978785/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-8c4d76b2be978785/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 10000 examples [00:00, 17230.64 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-8c4d76b2be978785/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Converting format of dataset:   0% 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-8c4d76b2be978785/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ea9c7c9160059ae1.arrow\n",
            "Converting format of dataset: 100% 10000/10000 [00:00<00:00, 16322.12 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-8c4d76b2be978785/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-233d11a134e51f18.arrow\n",
            "Running tokenizer on dataset: 100% 10000/10000 [00:17<00:00, 568.70 examples/s]\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 38437, 614, 2615, 311, 279, 2701, 7375, 510, 29, 13474, 3988, 25, 11047, 42879, 198, 7740, 7662, 25, 20517, 279, 47674, 3349, 198, 7740, 17693, 510, 220, 481, 4024, 9040, 320, 4082, 11, 2567, 1648, 576, 4024, 3349, 315, 279, 1509, 198, 220, 481, 11089, 46044, 320, 4082, 11, 2567, 1648, 576, 11414, 315, 11089, 271, 10253, 279, 2701, 3561, 421, 1667, 264, 5392, 510, 13874, 3989, 2512, 25, 5392, 829, 320, 603, 315, 508, 35597, 42879, 61261, 2512, 5571, 25, 279, 1946, 311, 279, 5392, 11, 304, 264, 4718, 3561, 14064, 279, 16494, 320, 68, 1302, 13, 54275, 4913, 1355, 788, 330, 14990, 1879, 497, 330, 2413, 21263, 4122, 788, 220, 20, 5541, 13874, 4292, 13874, 3989, 151645, 198, 151644, 872, 198, 40, 5485, 264, 8511, 429, 358, 14915, 13, 1084, 572, 13214, 32605, 518, 400, 17, 15, 15, 714, 432, 594, 389, 6278, 369, 220, 17, 15, 4, 1007, 13, 2980, 498, 3291, 752, 1246, 1753, 432, 686, 2783, 1283, 279, 11089, 30, 151645, 198, 151644, 77091, 198, 2512, 25, 11047, 42879, 198, 2512, 5571, 25, 5212, 9889, 9040, 788, 220, 17, 15, 15, 11, 330, 27359, 46044, 788, 220, 17, 15, 92, 151645, 198, 151644, 872, 198, 4913, 27359, 291, 9040, 788, 220, 16, 21, 15, 92, 151645, 198, 151644, 77091, 198, 785, 8511, 686, 2783, 498, 400, 16, 21, 15, 1283, 279, 220, 17, 15, 4, 11089, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.You have access to the following tools:\n",
            "> Tool Name: calculate_discount\n",
            "Tool Description: Calculate the discounted price\n",
            "Tool Args:\n",
            "  - original_price (number, required): The original price of the item\n",
            "  - discount_percentage (number, required): The percentage of discount\n",
            "\n",
            "Use the following format if using a tool:\n",
            "```\n",
            "Action: tool name (one of [calculate_discount]).\n",
            "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. ```{\"input\": \"hello world\", \"num_beams\": 5}```).\n",
            "```\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "I saw a dress that I liked. It was originally priced at $200 but it's on sale for 20% off. Can you tell me how much it will cost after the discount?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Action: calculate_discount\n",
            "Action Input: {\"original_price\": 200, \"discount_percentage\": 20}<|im_end|>\n",
            "<|im_start|>user\n",
            "{\"discounted_price\": 160}<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The dress will cost you $160 after the 20% discount.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2512, 25, 11047, 42879, 198, 2512, 5571, 25, 5212, 9889, 9040, 788, 220, 17, 15, 15, 11, 330, 27359, 46044, 788, 220, 17, 15, 92, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 8511, 686, 2783, 498, 400, 16, 21, 15, 1283, 279, 220, 17, 15, 4, 11089, 13, 151645]\n",
            "labels:\n",
            "Action: calculate_discount\n",
            "Action Input: {\"original_price\": 200, \"discount_percentage\": 20}<|im_end|>The dress will cost you $160 after the 20% discount.<|im_end|>\n",
            "[INFO|training_args.py:1838] 2024-01-24 05:38:11,311 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:568] 2024-01-24 05:38:12,556 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1706] 2024-01-24 05:38:12,958 >> ***** Running training *****\n",
            "[INFO|trainer.py:1707] 2024-01-24 05:38:12,959 >>   Num examples = 10,000\n",
            "[INFO|trainer.py:1708] 2024-01-24 05:38:12,959 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1709] 2024-01-24 05:38:12,959 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1712] 2024-01-24 05:38:12,959 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1713] 2024-01-24 05:38:12,959 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1714] 2024-01-24 05:38:12,959 >>   Total optimization steps = 1,875\n",
            "[INFO|trainer.py:1715] 2024-01-24 05:38:12,961 >>   Number of trainable parameters = 1,572,864\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 05:38:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.7449, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7449, 'learning_rate': 4.999912270696202e-05, 'epoch': 0.01}\n",
            "01/24/2024 05:39:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.6868, 'learning_rate': 4.9996e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6868, 'learning_rate': 4.9996490889419514e-05, 'epoch': 0.02}\n",
            "01/24/2024 05:40:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.6778, 'learning_rate': 4.9992e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6778, 'learning_rate': 4.99921047320825e-05, 'epoch': 0.02}\n",
            "01/24/2024 05:40:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.6774, 'learning_rate': 4.9986e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6774, 'learning_rate': 4.9985964542786614e-05, 'epoch': 0.03}\n",
            "01/24/2024 05:41:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.6884, 'learning_rate': 4.9978e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6884, 'learning_rate': 4.997807075247146e-05, 'epoch': 0.04}\n",
            "01/24/2024 05:42:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.7303, 'learning_rate': 4.9968e-05, 'epoch': 0.05}\n",
            "{'loss': 0.7303, 'learning_rate': 4.996842391515044e-05, 'epoch': 0.05}\n",
            "01/24/2024 05:42:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.6320, 'learning_rate': 4.9957e-05, 'epoch': 0.06}\n",
            "{'loss': 0.632, 'learning_rate': 4.9957024707871806e-05, 'epoch': 0.06}\n",
            "01/24/2024 05:43:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.5657, 'learning_rate': 4.9944e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5657, 'learning_rate': 4.994387393067117e-05, 'epoch': 0.06}\n",
            "01/24/2024 05:44:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.6167, 'learning_rate': 4.9929e-05, 'epoch': 0.07}\n",
            "{'loss': 0.6167, 'learning_rate': 4.992897250651535e-05, 'epoch': 0.07}\n",
            "01/24/2024 05:44:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.5504, 'learning_rate': 4.9912e-05, 'epoch': 0.08}\n",
            "{'loss': 0.5504, 'learning_rate': 4.991232148123761e-05, 'epoch': 0.08}\n",
            "01/24/2024 05:45:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.6526, 'learning_rate': 4.9894e-05, 'epoch': 0.09}\n",
            "{'loss': 0.6526, 'learning_rate': 4.9893922023464236e-05, 'epoch': 0.09}\n",
            "01/24/2024 05:46:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.7208, 'learning_rate': 4.9874e-05, 'epoch': 0.10}\n",
            "{'loss': 0.7208, 'learning_rate': 4.987377542453251e-05, 'epoch': 0.1}\n",
            "01/24/2024 05:47:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.5925, 'learning_rate': 4.9852e-05, 'epoch': 0.10}\n",
            "{'loss': 0.5925, 'learning_rate': 4.985188309840012e-05, 'epoch': 0.1}\n",
            "01/24/2024 05:48:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.7028, 'learning_rate': 4.9828e-05, 'epoch': 0.11}\n",
            "{'loss': 0.7028, 'learning_rate': 4.982824658154589e-05, 'epoch': 0.11}\n",
            "01/24/2024 05:48:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.5743, 'learning_rate': 4.9803e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5743, 'learning_rate': 4.980286753286195e-05, 'epoch': 0.12}\n",
            "01/24/2024 05:49:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.6398, 'learning_rate': 4.9776e-05, 'epoch': 0.13}\n",
            "{'loss': 0.6398, 'learning_rate': 4.977574773353732e-05, 'epoch': 0.13}\n",
            "01/24/2024 05:50:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.7025, 'learning_rate': 4.9747e-05, 'epoch': 0.14}\n",
            "{'loss': 0.7025, 'learning_rate': 4.9746889086932895e-05, 'epoch': 0.14}\n",
            "01/24/2024 05:50:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.5977, 'learning_rate': 4.9716e-05, 'epoch': 0.14}\n",
            "{'loss': 0.5977, 'learning_rate': 4.971629361844785e-05, 'epoch': 0.14}\n",
            "01/24/2024 05:51:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.6058, 'learning_rate': 4.9684e-05, 'epoch': 0.15}\n",
            "{'loss': 0.6058, 'learning_rate': 4.968396347537751e-05, 'epoch': 0.15}\n",
            "01/24/2024 05:52:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.6013, 'learning_rate': 4.9650e-05, 'epoch': 0.16}\n",
            "{'loss': 0.6013, 'learning_rate': 4.964990092676263e-05, 'epoch': 0.16}\n",
            "[INFO|trainer.py:2889] 2024-01-24 05:52:00,848 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-100\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 05:52:00,885 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 05:52:00,885 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-100/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 05:52:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.6615, 'learning_rate': 4.9614e-05, 'epoch': 0.17}\n",
            "{'loss': 0.6615, 'learning_rate': 4.9614108363230135e-05, 'epoch': 0.17}\n",
            "01/24/2024 05:53:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.6433, 'learning_rate': 4.9577e-05, 'epoch': 0.18}\n",
            "{'loss': 0.6433, 'learning_rate': 4.9576588296825386e-05, 'epoch': 0.18}\n",
            "01/24/2024 05:54:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.5750, 'learning_rate': 4.9537e-05, 'epoch': 0.18}\n",
            "{'loss': 0.575, 'learning_rate': 4.953734336083583e-05, 'epoch': 0.18}\n",
            "01/24/2024 05:54:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.5466, 'learning_rate': 4.9496e-05, 'epoch': 0.19}\n",
            "{'loss': 0.5466, 'learning_rate': 4.949637630960617e-05, 'epoch': 0.19}\n",
            "01/24/2024 05:55:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5832, 'learning_rate': 4.9454e-05, 'epoch': 0.20}\n",
            "{'loss': 0.5832, 'learning_rate': 4.9453690018345144e-05, 'epoch': 0.2}\n",
            "01/24/2024 05:56:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.6560, 'learning_rate': 4.9409e-05, 'epoch': 0.21}\n",
            "{'loss': 0.656, 'learning_rate': 4.940928748292363e-05, 'epoch': 0.21}\n",
            "01/24/2024 05:56:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.4955, 'learning_rate': 4.9363e-05, 'epoch': 0.22}\n",
            "{'loss': 0.4955, 'learning_rate': 4.9363171819664434e-05, 'epoch': 0.22}\n",
            "01/24/2024 05:57:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.5542, 'learning_rate': 4.9315e-05, 'epoch': 0.22}\n",
            "{'loss': 0.5542, 'learning_rate': 4.9315346265123594e-05, 'epoch': 0.22}\n",
            "01/24/2024 05:58:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.5616, 'learning_rate': 4.9266e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5616, 'learning_rate': 4.9265814175863186e-05, 'epoch': 0.23}\n",
            "01/24/2024 05:59:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.5623, 'learning_rate': 4.9215e-05, 'epoch': 0.24}\n",
            "{'loss': 0.5623, 'learning_rate': 4.9214579028215776e-05, 'epoch': 0.24}\n",
            "01/24/2024 05:59:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.6266, 'learning_rate': 4.9162e-05, 'epoch': 0.25}\n",
            "{'loss': 0.6266, 'learning_rate': 4.916164441804044e-05, 'epoch': 0.25}\n",
            "01/24/2024 06:00:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.5019, 'learning_rate': 4.9107e-05, 'epoch': 0.26}\n",
            "{'loss': 0.5019, 'learning_rate': 4.910701406047037e-05, 'epoch': 0.26}\n",
            "01/24/2024 06:01:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.5527, 'learning_rate': 4.9051e-05, 'epoch': 0.26}\n",
            "{'loss': 0.5527, 'learning_rate': 4.905069178965215e-05, 'epoch': 0.26}\n",
            "01/24/2024 06:01:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.5496, 'learning_rate': 4.8993e-05, 'epoch': 0.27}\n",
            "{'loss': 0.5496, 'learning_rate': 4.899268155847667e-05, 'epoch': 0.27}\n",
            "01/24/2024 06:02:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.5461, 'learning_rate': 4.8933e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5461, 'learning_rate': 4.893298743830168e-05, 'epoch': 0.28}\n",
            "01/24/2024 06:03:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.4738, 'learning_rate': 4.8872e-05, 'epoch': 0.29}\n",
            "{'loss': 0.4738, 'learning_rate': 4.887161361866608e-05, 'epoch': 0.29}\n",
            "01/24/2024 06:03:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.5276, 'learning_rate': 4.8809e-05, 'epoch': 0.30}\n",
            "{'loss': 0.5276, 'learning_rate': 4.880856440699582e-05, 'epoch': 0.3}\n",
            "01/24/2024 06:04:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.6091, 'learning_rate': 4.8744e-05, 'epoch': 0.30}\n",
            "{'loss': 0.6091, 'learning_rate': 4.874384422830167e-05, 'epoch': 0.3}\n",
            "01/24/2024 06:05:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.5578, 'learning_rate': 4.8677e-05, 'epoch': 0.31}\n",
            "{'loss': 0.5578, 'learning_rate': 4.867745762486861e-05, 'epoch': 0.31}\n",
            "01/24/2024 06:05:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.4748, 'learning_rate': 4.8609e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4748, 'learning_rate': 4.860940925593703e-05, 'epoch': 0.32}\n",
            "[INFO|trainer.py:2889] 2024-01-24 06:05:57,437 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-200\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 06:05:57,473 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 06:05:57,474 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-200/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 06:06:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.4447, 'learning_rate': 4.8540e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4447, 'learning_rate': 4.8539703897375755e-05, 'epoch': 0.33}\n",
            "01/24/2024 06:07:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.5204, 'learning_rate': 4.8468e-05, 'epoch': 0.34}\n",
            "{'loss': 0.5204, 'learning_rate': 4.846834644134686e-05, 'epoch': 0.34}\n",
            "01/24/2024 06:07:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.4447, 'learning_rate': 4.8395e-05, 'epoch': 0.34}\n",
            "{'loss': 0.4447, 'learning_rate': 4.839534189596228e-05, 'epoch': 0.34}\n",
            "01/24/2024 06:08:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.6128, 'learning_rate': 4.8321e-05, 'epoch': 0.35}\n",
            "{'loss': 0.6128, 'learning_rate': 4.832069538493237e-05, 'epoch': 0.35}\n",
            "01/24/2024 06:09:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.5137, 'learning_rate': 4.8244e-05, 'epoch': 0.36}\n",
            "{'loss': 0.5137, 'learning_rate': 4.8244412147206284e-05, 'epoch': 0.36}\n",
            "01/24/2024 06:09:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.4821, 'learning_rate': 4.8166e-05, 'epoch': 0.37}\n",
            "{'loss': 0.4821, 'learning_rate': 4.81664975366043e-05, 'epoch': 0.37}\n",
            "01/24/2024 06:10:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.4528, 'learning_rate': 4.8087e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4528, 'learning_rate': 4.808695702144206e-05, 'epoch': 0.38}\n",
            "01/24/2024 06:11:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.4971, 'learning_rate': 4.8006e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4971, 'learning_rate': 4.800579618414676e-05, 'epoch': 0.38}\n",
            "01/24/2024 06:11:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.3950, 'learning_rate': 4.7923e-05, 'epoch': 0.39}\n",
            "{'loss': 0.395, 'learning_rate': 4.7923020720865414e-05, 'epoch': 0.39}\n",
            "01/24/2024 06:12:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.4295, 'learning_rate': 4.7839e-05, 'epoch': 0.40}\n",
            "{'loss': 0.4295, 'learning_rate': 4.783863644106502e-05, 'epoch': 0.4}\n",
            "01/24/2024 06:13:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.5694, 'learning_rate': 4.7753e-05, 'epoch': 0.41}\n",
            "{'loss': 0.5694, 'learning_rate': 4.775264926712489e-05, 'epoch': 0.41}\n",
            "01/24/2024 06:13:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.4484, 'learning_rate': 4.7665e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4484, 'learning_rate': 4.7665065233920945e-05, 'epoch': 0.42}\n",
            "01/24/2024 06:14:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.6216, 'learning_rate': 4.7576e-05, 'epoch': 0.42}\n",
            "{'loss': 0.6216, 'learning_rate': 4.7575890488402185e-05, 'epoch': 0.42}\n",
            "01/24/2024 06:15:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.4289, 'learning_rate': 4.7485e-05, 'epoch': 0.43}\n",
            "{'loss': 0.4289, 'learning_rate': 4.7485131289159276e-05, 'epoch': 0.43}\n",
            "01/24/2024 06:15:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5106, 'learning_rate': 4.7393e-05, 'epoch': 0.44}\n",
            "{'loss': 0.5106, 'learning_rate': 4.7392794005985326e-05, 'epoch': 0.44}\n",
            "01/24/2024 06:16:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.4320, 'learning_rate': 4.7299e-05, 'epoch': 0.45}\n",
            "{'loss': 0.432, 'learning_rate': 4.7298885119428773e-05, 'epoch': 0.45}\n",
            "01/24/2024 06:17:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.5784, 'learning_rate': 4.7203e-05, 'epoch': 0.46}\n",
            "{'loss': 0.5784, 'learning_rate': 4.720341122033862e-05, 'epoch': 0.46}\n",
            "01/24/2024 06:17:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.5030, 'learning_rate': 4.7106e-05, 'epoch': 0.46}\n",
            "{'loss': 0.503, 'learning_rate': 4.710637900940181e-05, 'epoch': 0.46}\n",
            "01/24/2024 06:18:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.5054, 'learning_rate': 4.7008e-05, 'epoch': 0.47}\n",
            "{'loss': 0.5054, 'learning_rate': 4.7007795296673006e-05, 'epoch': 0.47}\n",
            "01/24/2024 06:19:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.6028, 'learning_rate': 4.6908e-05, 'epoch': 0.48}\n",
            "{'loss': 0.6028, 'learning_rate': 4.690766700109659e-05, 'epoch': 0.48}\n",
            "[INFO|trainer.py:2889] 2024-01-24 06:19:24,830 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-300\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 06:19:24,862 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 06:19:24,862 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-300/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 06:20:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.5606, 'learning_rate': 4.6806e-05, 'epoch': 0.49}\n",
            "{'loss': 0.5606, 'learning_rate': 4.68060011500211e-05, 'epoch': 0.49}\n",
            "01/24/2024 06:20:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.4924, 'learning_rate': 4.6703e-05, 'epoch': 0.50}\n",
            "{'loss': 0.4924, 'learning_rate': 4.670280487870598e-05, 'epoch': 0.5}\n",
            "01/24/2024 06:21:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.5380, 'learning_rate': 4.6598e-05, 'epoch': 0.50}\n",
            "{'loss': 0.538, 'learning_rate': 4.659808542982088e-05, 'epoch': 0.5}\n",
            "01/24/2024 06:22:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.5930, 'learning_rate': 4.6492e-05, 'epoch': 0.51}\n",
            "{'loss': 0.593, 'learning_rate': 4.649185015293728e-05, 'epoch': 0.51}\n",
            "01/24/2024 06:22:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.4461, 'learning_rate': 4.6384e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4461, 'learning_rate': 4.638410650401267e-05, 'epoch': 0.52}\n",
            "01/24/2024 06:23:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.4806, 'learning_rate': 4.6275e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4806, 'learning_rate': 4.6274862044867304e-05, 'epoch': 0.53}\n",
            "01/24/2024 06:24:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.6080, 'learning_rate': 4.6164e-05, 'epoch': 0.54}\n",
            "{'loss': 0.608, 'learning_rate': 4.616412444265345e-05, 'epoch': 0.54}\n",
            "01/24/2024 06:24:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5200, 'learning_rate': 4.6052e-05, 'epoch': 0.54}\n",
            "{'loss': 0.52, 'learning_rate': 4.605190146931731e-05, 'epoch': 0.54}\n",
            "01/24/2024 06:25:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.5199, 'learning_rate': 4.5938e-05, 'epoch': 0.55}\n",
            "{'loss': 0.5199, 'learning_rate': 4.593820100105355e-05, 'epoch': 0.55}\n",
            "01/24/2024 06:26:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.5270, 'learning_rate': 4.5823e-05, 'epoch': 0.56}\n",
            "{'loss': 0.527, 'learning_rate': 4.5823031017752485e-05, 'epoch': 0.56}\n",
            "01/24/2024 06:27:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.5460, 'learning_rate': 4.5706e-05, 'epoch': 0.57}\n",
            "{'loss': 0.546, 'learning_rate': 4.5706399602440106e-05, 'epoch': 0.57}\n",
            "01/24/2024 06:27:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.4134, 'learning_rate': 4.5588e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4134, 'learning_rate': 4.558831494071069e-05, 'epoch': 0.58}\n",
            "01/24/2024 06:28:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.5351, 'learning_rate': 4.5469e-05, 'epoch': 0.58}\n",
            "{'loss': 0.5351, 'learning_rate': 4.5468785320152365e-05, 'epoch': 0.58}\n",
            "01/24/2024 06:28:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.5235, 'learning_rate': 4.5348e-05, 'epoch': 0.59}\n",
            "{'loss': 0.5235, 'learning_rate': 4.534781912976546e-05, 'epoch': 0.59}\n",
            "01/24/2024 06:29:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.5733, 'learning_rate': 4.5225e-05, 'epoch': 0.60}\n",
            "{'loss': 0.5733, 'learning_rate': 4.522542485937369e-05, 'epoch': 0.6}\n",
            "01/24/2024 06:30:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.5303, 'learning_rate': 4.5102e-05, 'epoch': 0.61}\n",
            "{'loss': 0.5303, 'learning_rate': 4.510161109902837e-05, 'epoch': 0.61}\n",
            "01/24/2024 06:31:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.4843, 'learning_rate': 4.4976e-05, 'epoch': 0.62}\n",
            "{'loss': 0.4843, 'learning_rate': 4.4976386538405495e-05, 'epoch': 0.62}\n",
            "01/24/2024 06:31:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.4737, 'learning_rate': 4.4850e-05, 'epoch': 0.62}\n",
            "{'loss': 0.4737, 'learning_rate': 4.484975996619589e-05, 'epoch': 0.62}\n",
            "01/24/2024 06:32:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.4831, 'learning_rate': 4.4722e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4831, 'learning_rate': 4.4721740269488355e-05, 'epoch': 0.63}\n",
            "01/24/2024 06:33:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.6255, 'learning_rate': 4.4592e-05, 'epoch': 0.64}\n",
            "{'loss': 0.6255, 'learning_rate': 4.4592336433146e-05, 'epoch': 0.64}\n",
            "[INFO|trainer.py:2889] 2024-01-24 06:33:11,908 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-400\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 06:33:11,939 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 06:33:11,940 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-400/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 06:33:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5859, 'learning_rate': 4.4462e-05, 'epoch': 0.65}\n",
            "{'loss': 0.5859, 'learning_rate': 4.4461557539175594e-05, 'epoch': 0.65}\n",
            "01/24/2024 06:34:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.4424, 'learning_rate': 4.4329e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4424, 'learning_rate': 4.432941276609018e-05, 'epoch': 0.66}\n",
            "01/24/2024 06:35:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.4736, 'learning_rate': 4.4196e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4736, 'learning_rate': 4.4195911388264946e-05, 'epoch': 0.66}\n",
            "01/24/2024 06:35:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.5939, 'learning_rate': 4.4061e-05, 'epoch': 0.67}\n",
            "{'loss': 0.5939, 'learning_rate': 4.40610627752862e-05, 'epoch': 0.67}\n",
            "01/24/2024 06:36:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.5202, 'learning_rate': 4.3925e-05, 'epoch': 0.68}\n",
            "{'loss': 0.5202, 'learning_rate': 4.3924876391293915e-05, 'epoch': 0.68}\n",
            "01/24/2024 06:37:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.5513, 'learning_rate': 4.3787e-05, 'epoch': 0.69}\n",
            "{'loss': 0.5513, 'learning_rate': 4.3787361794317405e-05, 'epoch': 0.69}\n",
            "01/24/2024 06:38:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.6302, 'learning_rate': 4.3649e-05, 'epoch': 0.70}\n",
            "{'loss': 0.6302, 'learning_rate': 4.3648528635604556e-05, 'epoch': 0.7}\n",
            "01/24/2024 06:38:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.5520, 'learning_rate': 4.3508e-05, 'epoch': 0.70}\n",
            "{'loss': 0.552, 'learning_rate': 4.350838665894446e-05, 'epoch': 0.7}\n",
            "01/24/2024 06:39:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.5259, 'learning_rate': 4.3367e-05, 'epoch': 0.71}\n",
            "{'loss': 0.5259, 'learning_rate': 4.336694569998354e-05, 'epoch': 0.71}\n",
            "01/24/2024 06:40:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.6032, 'learning_rate': 4.3224e-05, 'epoch': 0.72}\n",
            "{'loss': 0.6032, 'learning_rate': 4.3224215685535294e-05, 'epoch': 0.72}\n",
            "01/24/2024 06:40:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.5866, 'learning_rate': 4.3080e-05, 'epoch': 0.73}\n",
            "{'loss': 0.5866, 'learning_rate': 4.3080206632883554e-05, 'epoch': 0.73}\n",
            "01/24/2024 06:41:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.4922, 'learning_rate': 4.2935e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4922, 'learning_rate': 4.293492864907947e-05, 'epoch': 0.74}\n",
            "01/24/2024 06:42:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.4315, 'learning_rate': 4.2788e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4315, 'learning_rate': 4.278839193023214e-05, 'epoch': 0.74}\n",
            "01/24/2024 06:42:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.5500, 'learning_rate': 4.2641e-05, 'epoch': 0.75}\n",
            "{'loss': 0.55, 'learning_rate': 4.264060676079302e-05, 'epoch': 0.75}\n",
            "01/24/2024 06:43:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.3959, 'learning_rate': 4.2492e-05, 'epoch': 0.76}\n",
            "{'loss': 0.3959, 'learning_rate': 4.249158351283414e-05, 'epoch': 0.76}\n",
            "01/24/2024 06:44:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.5045, 'learning_rate': 4.2341e-05, 'epoch': 0.77}\n",
            "{'loss': 0.5045, 'learning_rate': 4.234133264532012e-05, 'epoch': 0.77}\n",
            "01/24/2024 06:44:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5353, 'learning_rate': 4.2190e-05, 'epoch': 0.78}\n",
            "{'loss': 0.5353, 'learning_rate': 4.218986470337419e-05, 'epoch': 0.78}\n",
            "01/24/2024 06:45:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.4479, 'learning_rate': 4.2037e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4479, 'learning_rate': 4.2037190317538e-05, 'epoch': 0.78}\n",
            "01/24/2024 06:46:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.5137, 'learning_rate': 4.1883e-05, 'epoch': 0.79}\n",
            "{'loss': 0.5137, 'learning_rate': 4.188332020302561e-05, 'epoch': 0.79}\n",
            "01/24/2024 06:46:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.5533, 'learning_rate': 4.1728e-05, 'epoch': 0.80}\n",
            "{'loss': 0.5533, 'learning_rate': 4.172826515897146e-05, 'epoch': 0.8}\n",
            "[INFO|trainer.py:2889] 2024-01-24 06:46:56,595 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-500\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 06:46:56,627 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 06:46:56,627 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 06:47:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.6954, 'learning_rate': 4.1572e-05, 'epoch': 0.81}\n",
            "{'loss': 0.6954, 'learning_rate': 4.157203606767238e-05, 'epoch': 0.81}\n",
            "01/24/2024 06:48:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.5934, 'learning_rate': 4.1415e-05, 'epoch': 0.82}\n",
            "{'loss': 0.5934, 'learning_rate': 4.1414643893823914e-05, 'epoch': 0.82}\n",
            "01/24/2024 06:49:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.5967, 'learning_rate': 4.1256e-05, 'epoch': 0.82}\n",
            "{'loss': 0.5967, 'learning_rate': 4.125609968375072e-05, 'epoch': 0.82}\n",
            "01/24/2024 06:49:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5610, 'learning_rate': 4.1096e-05, 'epoch': 0.83}\n",
            "{'loss': 0.561, 'learning_rate': 4.109641456463135e-05, 'epoch': 0.83}\n",
            "01/24/2024 06:50:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.5499, 'learning_rate': 4.0936e-05, 'epoch': 0.84}\n",
            "{'loss': 0.5499, 'learning_rate': 4.093559974371725e-05, 'epoch': 0.84}\n",
            "01/24/2024 06:51:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.5078, 'learning_rate': 4.0774e-05, 'epoch': 0.85}\n",
            "{'loss': 0.5078, 'learning_rate': 4.077366650754624e-05, 'epoch': 0.85}\n",
            "01/24/2024 06:52:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.5763, 'learning_rate': 4.0611e-05, 'epoch': 0.86}\n",
            "{'loss': 0.5763, 'learning_rate': 4.0610626221150394e-05, 'epoch': 0.86}\n",
            "01/24/2024 06:52:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.6261, 'learning_rate': 4.0446e-05, 'epoch': 0.86}\n",
            "{'loss': 0.6261, 'learning_rate': 4.044649032725836e-05, 'epoch': 0.86}\n",
            "01/24/2024 06:53:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5499, 'learning_rate': 4.0281e-05, 'epoch': 0.87}\n",
            "{'loss': 0.5499, 'learning_rate': 4.028127034549229e-05, 'epoch': 0.87}\n",
            "01/24/2024 06:54:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.5601, 'learning_rate': 4.0115e-05, 'epoch': 0.88}\n",
            "{'loss': 0.5601, 'learning_rate': 4.011497787155938e-05, 'epoch': 0.88}\n",
            "01/24/2024 06:54:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.4842, 'learning_rate': 3.9948e-05, 'epoch': 0.89}\n",
            "{'loss': 0.4842, 'learning_rate': 3.9947624576437975e-05, 'epoch': 0.89}\n",
            "01/24/2024 06:55:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.4992, 'learning_rate': 3.9779e-05, 'epoch': 0.90}\n",
            "{'loss': 0.4992, 'learning_rate': 3.977922220555855e-05, 'epoch': 0.9}\n",
            "01/24/2024 06:56:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.6235, 'learning_rate': 3.9610e-05, 'epoch': 0.90}\n",
            "{'loss': 0.6235, 'learning_rate': 3.960978257797931e-05, 'epoch': 0.9}\n",
            "01/24/2024 06:57:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.4714, 'learning_rate': 3.9439e-05, 'epoch': 0.91}\n",
            "{'loss': 0.4714, 'learning_rate': 3.943931758555669e-05, 'epoch': 0.91}\n",
            "01/24/2024 06:57:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.5629, 'learning_rate': 3.9268e-05, 'epoch': 0.92}\n",
            "{'loss': 0.5629, 'learning_rate': 3.92678391921108e-05, 'epoch': 0.92}\n",
            "01/24/2024 06:58:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.5499, 'learning_rate': 3.9095e-05, 'epoch': 0.93}\n",
            "{'loss': 0.5499, 'learning_rate': 3.909535943258567e-05, 'epoch': 0.93}\n",
            "01/24/2024 06:59:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.5764, 'learning_rate': 3.8922e-05, 'epoch': 0.94}\n",
            "{'loss': 0.5764, 'learning_rate': 3.8921890412204705e-05, 'epoch': 0.94}\n",
            "01/24/2024 06:59:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5937, 'learning_rate': 3.8747e-05, 'epoch': 0.94}\n",
            "{'loss': 0.5937, 'learning_rate': 3.8747444305621e-05, 'epoch': 0.94}\n",
            "01/24/2024 07:00:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.5832, 'learning_rate': 3.8572e-05, 'epoch': 0.95}\n",
            "{'loss': 0.5832, 'learning_rate': 3.8572033356062943e-05, 'epoch': 0.95}\n",
            "01/24/2024 07:01:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.4241, 'learning_rate': 3.8396e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4241, 'learning_rate': 3.8395669874474915e-05, 'epoch': 0.96}\n",
            "[INFO|trainer.py:2889] 2024-01-24 07:01:14,856 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-600\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 07:01:14,888 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 07:01:14,888 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-600/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 07:02:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.6510, 'learning_rate': 3.8218e-05, 'epoch': 0.97}\n",
            "{'loss': 0.651, 'learning_rate': 3.821836623865329e-05, 'epoch': 0.97}\n",
            "01/24/2024 07:02:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.4758, 'learning_rate': 3.8040e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4758, 'learning_rate': 3.80401348923777e-05, 'epoch': 0.98}\n",
            "01/24/2024 07:03:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.5605, 'learning_rate': 3.7861e-05, 'epoch': 0.98}\n",
            "{'loss': 0.5605, 'learning_rate': 3.786098834453766e-05, 'epoch': 0.98}\n",
            "01/24/2024 07:04:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.5596, 'learning_rate': 3.7681e-05, 'epoch': 0.99}\n",
            "{'loss': 0.5596, 'learning_rate': 3.7680939168254733e-05, 'epoch': 0.99}\n",
            "01/24/2024 07:04:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.5618, 'learning_rate': 3.7500e-05, 'epoch': 1.00}\n",
            "{'loss': 0.5618, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}\n",
            "01/24/2024 07:05:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.4743, 'learning_rate': 3.7318e-05, 'epoch': 1.01}\n",
            "{'loss': 0.4743, 'learning_rate': 3.731818353870729e-05, 'epoch': 1.01}\n",
            "01/24/2024 07:06:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.5215, 'learning_rate': 3.7136e-05, 'epoch': 1.02}\n",
            "{'loss': 0.5215, 'learning_rate': 3.713550254488185e-05, 'epoch': 1.02}\n",
            "01/24/2024 07:06:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5129, 'learning_rate': 3.6952e-05, 'epoch': 1.02}\n",
            "{'loss': 0.5129, 'learning_rate': 3.695196983970481e-05, 'epoch': 1.02}\n",
            "01/24/2024 07:07:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.6274, 'learning_rate': 3.6768e-05, 'epoch': 1.03}\n",
            "{'loss': 0.6274, 'learning_rate': 3.6767598304133324e-05, 'epoch': 1.03}\n",
            "01/24/2024 07:08:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.4953, 'learning_rate': 3.6582e-05, 'epoch': 1.04}\n",
            "{'loss': 0.4953, 'learning_rate': 3.6582400877996546e-05, 'epoch': 1.04}\n",
            "01/24/2024 07:08:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.5109, 'learning_rate': 3.6396e-05, 'epoch': 1.05}\n",
            "{'loss': 0.5109, 'learning_rate': 3.639639055908751e-05, 'epoch': 1.05}\n",
            "01/24/2024 07:09:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.5284, 'learning_rate': 3.6210e-05, 'epoch': 1.06}\n",
            "{'loss': 0.5284, 'learning_rate': 3.6209580402250815e-05, 'epoch': 1.06}\n",
            "01/24/2024 07:10:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.5749, 'learning_rate': 3.6022e-05, 'epoch': 1.06}\n",
            "{'loss': 0.5749, 'learning_rate': 3.602198351846647e-05, 'epoch': 1.06}\n",
            "01/24/2024 07:11:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.5182, 'learning_rate': 3.5834e-05, 'epoch': 1.07}\n",
            "{'loss': 0.5182, 'learning_rate': 3.5833613073929684e-05, 'epoch': 1.07}\n",
            "01/24/2024 07:11:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.4321, 'learning_rate': 3.5644e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4321, 'learning_rate': 3.564448228912682e-05, 'epoch': 1.08}\n",
            "01/24/2024 07:12:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.4244, 'learning_rate': 3.5455e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4244, 'learning_rate': 3.545460443790753e-05, 'epoch': 1.09}\n",
            "01/24/2024 07:13:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.4786, 'learning_rate': 3.5264e-05, 'epoch': 1.10}\n",
            "{'loss': 0.4786, 'learning_rate': 3.52639928465532e-05, 'epoch': 1.1}\n",
            "01/24/2024 07:13:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5265, 'learning_rate': 3.5073e-05, 'epoch': 1.10}\n",
            "{'loss': 0.5265, 'learning_rate': 3.507266089284157e-05, 'epoch': 1.1}\n",
            "01/24/2024 07:14:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.4719, 'learning_rate': 3.4881e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4719, 'learning_rate': 3.488062200510791e-05, 'epoch': 1.11}\n",
            "01/24/2024 07:15:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.3861, 'learning_rate': 3.4688e-05, 'epoch': 1.12}\n",
            "{'loss': 0.3861, 'learning_rate': 3.4687889661302576e-05, 'epoch': 1.12}\n",
            "[INFO|trainer.py:2889] 2024-01-24 07:15:06,949 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-700\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 07:15:06,983 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-700/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 07:15:06,983 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-700/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 07:15:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.4969, 'learning_rate': 3.4494e-05, 'epoch': 1.13}\n",
            "{'loss': 0.4969, 'learning_rate': 3.4494477388045035e-05, 'epoch': 1.13}\n",
            "01/24/2024 07:16:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.4084, 'learning_rate': 3.4300e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4084, 'learning_rate': 3.430039875967454e-05, 'epoch': 1.14}\n",
            "01/24/2024 07:17:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.4407, 'learning_rate': 3.4106e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4407, 'learning_rate': 3.410566739729746e-05, 'epoch': 1.14}\n",
            "01/24/2024 07:17:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.4009, 'learning_rate': 3.3910e-05, 'epoch': 1.15}\n",
            "{'loss': 0.4009, 'learning_rate': 3.3910296967831266e-05, 'epoch': 1.15}\n",
            "01/24/2024 07:18:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5802, 'learning_rate': 3.3714e-05, 'epoch': 1.16}\n",
            "{'loss': 0.5802, 'learning_rate': 3.3714301183045385e-05, 'epoch': 1.16}\n",
            "01/24/2024 07:19:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.5946, 'learning_rate': 3.3518e-05, 'epoch': 1.17}\n",
            "{'loss': 0.5946, 'learning_rate': 3.35176937985988e-05, 'epoch': 1.17}\n",
            "01/24/2024 07:19:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5467, 'learning_rate': 3.3320e-05, 'epoch': 1.18}\n",
            "{'loss': 0.5467, 'learning_rate': 3.332048861307467e-05, 'epoch': 1.18}\n",
            "01/24/2024 07:20:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.4886, 'learning_rate': 3.3123e-05, 'epoch': 1.18}\n",
            "{'loss': 0.4886, 'learning_rate': 3.312269946701191e-05, 'epoch': 1.18}\n",
            "01/24/2024 07:21:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.4790, 'learning_rate': 3.2924e-05, 'epoch': 1.19}\n",
            "{'loss': 0.479, 'learning_rate': 3.29243402419338e-05, 'epoch': 1.19}\n",
            "01/24/2024 07:21:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.5493, 'learning_rate': 3.2725e-05, 'epoch': 1.20}\n",
            "{'loss': 0.5493, 'learning_rate': 3.272542485937369e-05, 'epoch': 1.2}\n",
            "01/24/2024 07:22:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.4433, 'learning_rate': 3.2526e-05, 'epoch': 1.21}\n",
            "{'loss': 0.4433, 'learning_rate': 3.2525967279898015e-05, 'epoch': 1.21}\n",
            "01/24/2024 07:23:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.4564, 'learning_rate': 3.2326e-05, 'epoch': 1.22}\n",
            "{'loss': 0.4564, 'learning_rate': 3.2325981502126433e-05, 'epoch': 1.22}\n",
            "01/24/2024 07:23:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.5299, 'learning_rate': 3.2125e-05, 'epoch': 1.22}\n",
            "{'loss': 0.5299, 'learning_rate': 3.21254815617494e-05, 'epoch': 1.22}\n",
            "01/24/2024 07:24:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.4719, 'learning_rate': 3.1924e-05, 'epoch': 1.23}\n",
            "{'loss': 0.4719, 'learning_rate': 3.192448153054306e-05, 'epoch': 1.23}\n",
            "01/24/2024 07:25:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.5122, 'learning_rate': 3.1723e-05, 'epoch': 1.24}\n",
            "{'loss': 0.5122, 'learning_rate': 3.172299551538164e-05, 'epoch': 1.24}\n",
            "01/24/2024 07:26:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.5524, 'learning_rate': 3.1521e-05, 'epoch': 1.25}\n",
            "{'loss': 0.5524, 'learning_rate': 3.152103765724743e-05, 'epoch': 1.25}\n",
            "01/24/2024 07:26:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.4525, 'learning_rate': 3.1319e-05, 'epoch': 1.26}\n",
            "{'loss': 0.4525, 'learning_rate': 3.1318622130238236e-05, 'epoch': 1.26}\n",
            "01/24/2024 07:27:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.5275, 'learning_rate': 3.1116e-05, 'epoch': 1.26}\n",
            "{'loss': 0.5275, 'learning_rate': 3.111576314057268e-05, 'epoch': 1.26}\n",
            "01/24/2024 07:28:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.4803, 'learning_rate': 3.0912e-05, 'epoch': 1.27}\n",
            "{'loss': 0.4803, 'learning_rate': 3.091247492559312e-05, 'epoch': 1.27}\n",
            "01/24/2024 07:28:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.5528, 'learning_rate': 3.0709e-05, 'epoch': 1.28}\n",
            "{'loss': 0.5528, 'learning_rate': 3.0708771752766394e-05, 'epoch': 1.28}\n",
            "[INFO|trainer.py:2889] 2024-01-24 07:28:52,119 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-800\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 07:28:52,150 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 07:28:52,151 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-800/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 07:29:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.5482, 'learning_rate': 3.0505e-05, 'epoch': 1.29}\n",
            "{'loss': 0.5482, 'learning_rate': 3.050466791868254e-05, 'epoch': 1.29}\n",
            "01/24/2024 07:30:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.5679, 'learning_rate': 3.0300e-05, 'epoch': 1.30}\n",
            "{'loss': 0.5679, 'learning_rate': 3.0300177748051373e-05, 'epoch': 1.3}\n",
            "01/24/2024 07:31:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.5807, 'learning_rate': 3.0095e-05, 'epoch': 1.30}\n",
            "{'loss': 0.5807, 'learning_rate': 3.0095315592697126e-05, 'epoch': 1.3}\n",
            "01/24/2024 07:31:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.5282, 'learning_rate': 2.9890e-05, 'epoch': 1.31}\n",
            "{'loss': 0.5282, 'learning_rate': 2.9890095830551207e-05, 'epoch': 1.31}\n",
            "01/24/2024 07:32:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.4569, 'learning_rate': 2.9685e-05, 'epoch': 1.32}\n",
            "{'loss': 0.4569, 'learning_rate': 2.9684532864643122e-05, 'epoch': 1.32}\n",
            "01/24/2024 07:33:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.4665, 'learning_rate': 2.9479e-05, 'epoch': 1.33}\n",
            "{'loss': 0.4665, 'learning_rate': 2.9478641122089562e-05, 'epoch': 1.33}\n",
            "01/24/2024 07:33:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.5488, 'learning_rate': 2.9272e-05, 'epoch': 1.34}\n",
            "{'loss': 0.5488, 'learning_rate': 2.9272435053081922e-05, 'epoch': 1.34}\n",
            "01/24/2024 07:34:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.6400, 'learning_rate': 2.9066e-05, 'epoch': 1.34}\n",
            "{'loss': 0.64, 'learning_rate': 2.9065929129872094e-05, 'epoch': 1.34}\n",
            "01/24/2024 07:35:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.4899, 'learning_rate': 2.8859e-05, 'epoch': 1.35}\n",
            "{'loss': 0.4899, 'learning_rate': 2.8859137845756784e-05, 'epoch': 1.35}\n",
            "01/24/2024 07:36:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.4926, 'learning_rate': 2.8652e-05, 'epoch': 1.36}\n",
            "{'loss': 0.4926, 'learning_rate': 2.8652075714060295e-05, 'epoch': 1.36}\n",
            "01/24/2024 07:36:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5938, 'learning_rate': 2.8445e-05, 'epoch': 1.37}\n",
            "{'loss': 0.5938, 'learning_rate': 2.844475726711595e-05, 'epoch': 1.37}\n",
            "01/24/2024 07:37:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.4130, 'learning_rate': 2.8237e-05, 'epoch': 1.38}\n",
            "{'loss': 0.413, 'learning_rate': 2.8237197055246172e-05, 'epoch': 1.38}\n",
            "01/24/2024 07:38:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.5618, 'learning_rate': 2.8029e-05, 'epoch': 1.38}\n",
            "{'loss': 0.5618, 'learning_rate': 2.8029409645741267e-05, 'epoch': 1.38}\n",
            "01/24/2024 07:39:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.5139, 'learning_rate': 2.7821e-05, 'epoch': 1.39}\n",
            "{'loss': 0.5139, 'learning_rate': 2.782140962183704e-05, 'epoch': 1.39}\n",
            "01/24/2024 07:39:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.5533, 'learning_rate': 2.7613e-05, 'epoch': 1.40}\n",
            "{'loss': 0.5533, 'learning_rate': 2.761321158169134e-05, 'epoch': 1.4}\n",
            "01/24/2024 07:40:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5654, 'learning_rate': 2.7405e-05, 'epoch': 1.41}\n",
            "{'loss': 0.5654, 'learning_rate': 2.7404830137359444e-05, 'epoch': 1.41}\n",
            "01/24/2024 07:41:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.4938, 'learning_rate': 2.7196e-05, 'epoch': 1.42}\n",
            "{'loss': 0.4938, 'learning_rate': 2.7196279913768584e-05, 'epoch': 1.42}\n",
            "01/24/2024 07:42:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.6001, 'learning_rate': 2.6988e-05, 'epoch': 1.42}\n",
            "{'loss': 0.6001, 'learning_rate': 2.6987575547691497e-05, 'epoch': 1.42}\n",
            "01/24/2024 07:42:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.4571, 'learning_rate': 2.6779e-05, 'epoch': 1.43}\n",
            "{'loss': 0.4571, 'learning_rate': 2.6778731686719178e-05, 'epoch': 1.43}\n",
            "01/24/2024 07:43:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.5948, 'learning_rate': 2.6570e-05, 'epoch': 1.44}\n",
            "{'loss': 0.5948, 'learning_rate': 2.656976298823284e-05, 'epoch': 1.44}\n",
            "[INFO|trainer.py:2889] 2024-01-24 07:43:21,630 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-900\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 07:43:21,663 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 07:43:21,663 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-900/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 07:43:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.4093, 'learning_rate': 2.6361e-05, 'epoch': 1.45}\n",
            "{'loss': 0.4093, 'learning_rate': 2.636068411837523e-05, 'epoch': 1.45}\n",
            "01/24/2024 07:44:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.5142, 'learning_rate': 2.6152e-05, 'epoch': 1.46}\n",
            "{'loss': 0.5142, 'learning_rate': 2.615150975102131e-05, 'epoch': 1.46}\n",
            "01/24/2024 07:45:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.3715, 'learning_rate': 2.5942e-05, 'epoch': 1.46}\n",
            "{'loss': 0.3715, 'learning_rate': 2.594225456674837e-05, 'epoch': 1.46}\n",
            "01/24/2024 07:45:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5102, 'learning_rate': 2.5733e-05, 'epoch': 1.47}\n",
            "{'loss': 0.5102, 'learning_rate': 2.5732933251805713e-05, 'epoch': 1.47}\n",
            "01/24/2024 07:46:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.5106, 'learning_rate': 2.5524e-05, 'epoch': 1.48}\n",
            "{'loss': 0.5106, 'learning_rate': 2.5523560497083926e-05, 'epoch': 1.48}\n",
            "01/24/2024 07:47:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.5180, 'learning_rate': 2.5314e-05, 'epoch': 1.49}\n",
            "{'loss': 0.518, 'learning_rate': 2.531415099708382e-05, 'epoch': 1.49}\n",
            "01/24/2024 07:48:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.4313, 'learning_rate': 2.5105e-05, 'epoch': 1.50}\n",
            "{'loss': 0.4313, 'learning_rate': 2.51047194488851e-05, 'epoch': 1.5}\n",
            "01/24/2024 07:48:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.5281, 'learning_rate': 2.4895e-05, 'epoch': 1.50}\n",
            "{'loss': 0.5281, 'learning_rate': 2.4895280551114907e-05, 'epoch': 1.5}\n",
            "01/24/2024 07:49:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5190, 'learning_rate': 2.4686e-05, 'epoch': 1.51}\n",
            "{'loss': 0.519, 'learning_rate': 2.4685849002916183e-05, 'epoch': 1.51}\n",
            "01/24/2024 07:50:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.4759, 'learning_rate': 2.4476e-05, 'epoch': 1.52}\n",
            "{'loss': 0.4759, 'learning_rate': 2.447643950291608e-05, 'epoch': 1.52}\n",
            "01/24/2024 07:50:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.6156, 'learning_rate': 2.4267e-05, 'epoch': 1.53}\n",
            "{'loss': 0.6156, 'learning_rate': 2.4267066748194296e-05, 'epoch': 1.53}\n",
            "01/24/2024 07:51:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.4714, 'learning_rate': 2.4058e-05, 'epoch': 1.54}\n",
            "{'loss': 0.4714, 'learning_rate': 2.4057745433251635e-05, 'epoch': 1.54}\n",
            "01/24/2024 07:52:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.3695, 'learning_rate': 2.3848e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3695, 'learning_rate': 2.384849024897869e-05, 'epoch': 1.54}\n",
            "01/24/2024 07:52:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.4655, 'learning_rate': 2.3639e-05, 'epoch': 1.55}\n",
            "{'loss': 0.4655, 'learning_rate': 2.3639315881624777e-05, 'epoch': 1.55}\n",
            "01/24/2024 07:53:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.3847, 'learning_rate': 2.3430e-05, 'epoch': 1.56}\n",
            "{'loss': 0.3847, 'learning_rate': 2.3430237011767167e-05, 'epoch': 1.56}\n",
            "01/24/2024 07:54:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.6598, 'learning_rate': 2.3221e-05, 'epoch': 1.57}\n",
            "{'loss': 0.6598, 'learning_rate': 2.3221268313280838e-05, 'epoch': 1.57}\n",
            "01/24/2024 07:54:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5030, 'learning_rate': 2.3012e-05, 'epoch': 1.58}\n",
            "{'loss': 0.503, 'learning_rate': 2.301242445230851e-05, 'epoch': 1.58}\n",
            "01/24/2024 07:55:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.5166, 'learning_rate': 2.2804e-05, 'epoch': 1.58}\n",
            "{'loss': 0.5166, 'learning_rate': 2.280372008623142e-05, 'epoch': 1.58}\n",
            "01/24/2024 07:56:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.5348, 'learning_rate': 2.2595e-05, 'epoch': 1.59}\n",
            "{'loss': 0.5348, 'learning_rate': 2.2595169862640568e-05, 'epoch': 1.59}\n",
            "01/24/2024 07:56:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.5267, 'learning_rate': 2.2387e-05, 'epoch': 1.60}\n",
            "{'loss': 0.5267, 'learning_rate': 2.238678841830867e-05, 'epoch': 1.6}\n",
            "[INFO|trainer.py:2889] 2024-01-24 07:56:57,069 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1000\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 07:56:57,120 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 07:56:57,121 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 07:57:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.6056, 'learning_rate': 2.2179e-05, 'epoch': 1.61}\n",
            "{'loss': 0.6056, 'learning_rate': 2.217859037816296e-05, 'epoch': 1.61}\n",
            "01/24/2024 07:58:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.4980, 'learning_rate': 2.1971e-05, 'epoch': 1.62}\n",
            "{'loss': 0.498, 'learning_rate': 2.1970590354258745e-05, 'epoch': 1.62}\n",
            "01/24/2024 07:59:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.4943, 'learning_rate': 2.1763e-05, 'epoch': 1.62}\n",
            "{'loss': 0.4943, 'learning_rate': 2.176280294475383e-05, 'epoch': 1.62}\n",
            "01/24/2024 07:59:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.4746, 'learning_rate': 2.1555e-05, 'epoch': 1.63}\n",
            "{'loss': 0.4746, 'learning_rate': 2.155524273288405e-05, 'epoch': 1.63}\n",
            "01/24/2024 08:00:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.5215, 'learning_rate': 2.1348e-05, 'epoch': 1.64}\n",
            "{'loss': 0.5215, 'learning_rate': 2.1347924285939714e-05, 'epoch': 1.64}\n",
            "01/24/2024 08:01:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.4136, 'learning_rate': 2.1141e-05, 'epoch': 1.65}\n",
            "{'loss': 0.4136, 'learning_rate': 2.114086215424322e-05, 'epoch': 1.65}\n",
            "01/24/2024 08:01:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.5378, 'learning_rate': 2.0934e-05, 'epoch': 1.66}\n",
            "{'loss': 0.5378, 'learning_rate': 2.0934070870127912e-05, 'epoch': 1.66}\n",
            "01/24/2024 08:02:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.3863, 'learning_rate': 2.0728e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3863, 'learning_rate': 2.0727564946918087e-05, 'epoch': 1.66}\n",
            "01/24/2024 08:03:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.5438, 'learning_rate': 2.0521e-05, 'epoch': 1.67}\n",
            "{'loss': 0.5438, 'learning_rate': 2.0521358877910444e-05, 'epoch': 1.67}\n",
            "01/24/2024 08:03:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.4178, 'learning_rate': 2.0315e-05, 'epoch': 1.68}\n",
            "{'loss': 0.4178, 'learning_rate': 2.031546713535688e-05, 'epoch': 1.68}\n",
            "01/24/2024 08:04:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.5791, 'learning_rate': 2.0110e-05, 'epoch': 1.69}\n",
            "{'loss': 0.5791, 'learning_rate': 2.01099041694488e-05, 'epoch': 1.69}\n",
            "01/24/2024 08:05:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.5406, 'learning_rate': 1.9905e-05, 'epoch': 1.70}\n",
            "{'loss': 0.5406, 'learning_rate': 1.9904684407302883e-05, 'epoch': 1.7}\n",
            "01/24/2024 08:06:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.4938, 'learning_rate': 1.9700e-05, 'epoch': 1.70}\n",
            "{'loss': 0.4938, 'learning_rate': 1.969982225194864e-05, 'epoch': 1.7}\n",
            "01/24/2024 08:06:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.5723, 'learning_rate': 1.9495e-05, 'epoch': 1.71}\n",
            "{'loss': 0.5723, 'learning_rate': 1.9495332081317464e-05, 'epoch': 1.71}\n",
            "01/24/2024 08:07:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.5141, 'learning_rate': 1.9291e-05, 'epoch': 1.72}\n",
            "{'loss': 0.5141, 'learning_rate': 1.9291228247233605e-05, 'epoch': 1.72}\n",
            "01/24/2024 08:08:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.6586, 'learning_rate': 1.9088e-05, 'epoch': 1.73}\n",
            "{'loss': 0.6586, 'learning_rate': 1.908752507440689e-05, 'epoch': 1.73}\n",
            "01/24/2024 08:08:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.4459, 'learning_rate': 1.8884e-05, 'epoch': 1.74}\n",
            "{'loss': 0.4459, 'learning_rate': 1.888423685942732e-05, 'epoch': 1.74}\n",
            "01/24/2024 08:09:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.4498, 'learning_rate': 1.8681e-05, 'epoch': 1.74}\n",
            "{'loss': 0.4498, 'learning_rate': 1.868137786976177e-05, 'epoch': 1.74}\n",
            "01/24/2024 08:10:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.5300, 'learning_rate': 1.8479e-05, 'epoch': 1.75}\n",
            "{'loss': 0.53, 'learning_rate': 1.8478962342752583e-05, 'epoch': 1.75}\n",
            "01/24/2024 08:11:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.4499, 'learning_rate': 1.8277e-05, 'epoch': 1.76}\n",
            "{'loss': 0.4499, 'learning_rate': 1.827700448461836e-05, 'epoch': 1.76}\n",
            "[INFO|trainer.py:2889] 2024-01-24 08:11:07,604 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1100\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 08:11:07,640 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 08:11:07,640 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1100/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 08:11:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.4694, 'learning_rate': 1.8076e-05, 'epoch': 1.77}\n",
            "{'loss': 0.4694, 'learning_rate': 1.807551846945694e-05, 'epoch': 1.77}\n",
            "01/24/2024 08:12:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.4916, 'learning_rate': 1.7875e-05, 'epoch': 1.78}\n",
            "{'loss': 0.4916, 'learning_rate': 1.7874518438250597e-05, 'epoch': 1.78}\n",
            "01/24/2024 08:13:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.4854, 'learning_rate': 1.7674e-05, 'epoch': 1.78}\n",
            "{'loss': 0.4854, 'learning_rate': 1.767401849787357e-05, 'epoch': 1.78}\n",
            "01/24/2024 08:13:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.5432, 'learning_rate': 1.7474e-05, 'epoch': 1.79}\n",
            "{'loss': 0.5432, 'learning_rate': 1.747403272010199e-05, 'epoch': 1.79}\n",
            "01/24/2024 08:14:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.5346, 'learning_rate': 1.7275e-05, 'epoch': 1.80}\n",
            "{'loss': 0.5346, 'learning_rate': 1.7274575140626318e-05, 'epoch': 1.8}\n",
            "01/24/2024 08:15:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.4300, 'learning_rate': 1.7076e-05, 'epoch': 1.81}\n",
            "{'loss': 0.43, 'learning_rate': 1.7075659758066208e-05, 'epoch': 1.81}\n",
            "01/24/2024 08:16:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.5068, 'learning_rate': 1.6877e-05, 'epoch': 1.82}\n",
            "{'loss': 0.5068, 'learning_rate': 1.6877300532988094e-05, 'epoch': 1.82}\n",
            "01/24/2024 08:16:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.5047, 'learning_rate': 1.6680e-05, 'epoch': 1.82}\n",
            "{'loss': 0.5047, 'learning_rate': 1.6679511386925337e-05, 'epoch': 1.82}\n",
            "01/24/2024 08:17:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.5154, 'learning_rate': 1.6482e-05, 'epoch': 1.83}\n",
            "{'loss': 0.5154, 'learning_rate': 1.648230620140121e-05, 'epoch': 1.83}\n",
            "01/24/2024 08:18:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.4331, 'learning_rate': 1.6286e-05, 'epoch': 1.84}\n",
            "{'loss': 0.4331, 'learning_rate': 1.6285698816954624e-05, 'epoch': 1.84}\n",
            "01/24/2024 08:18:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.5545, 'learning_rate': 1.6090e-05, 'epoch': 1.85}\n",
            "{'loss': 0.5545, 'learning_rate': 1.6089703032168733e-05, 'epoch': 1.85}\n",
            "01/24/2024 08:19:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.4886, 'learning_rate': 1.5894e-05, 'epoch': 1.86}\n",
            "{'loss': 0.4886, 'learning_rate': 1.5894332602702545e-05, 'epoch': 1.86}\n",
            "01/24/2024 08:20:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.5131, 'learning_rate': 1.5700e-05, 'epoch': 1.86}\n",
            "{'loss': 0.5131, 'learning_rate': 1.5699601240325474e-05, 'epoch': 1.86}\n",
            "01/24/2024 08:20:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.5156, 'learning_rate': 1.5506e-05, 'epoch': 1.87}\n",
            "{'loss': 0.5156, 'learning_rate': 1.5505522611954975e-05, 'epoch': 1.87}\n",
            "01/24/2024 08:21:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.5344, 'learning_rate': 1.5312e-05, 'epoch': 1.88}\n",
            "{'loss': 0.5344, 'learning_rate': 1.5312110338697426e-05, 'epoch': 1.88}\n",
            "01/24/2024 08:22:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.5501, 'learning_rate': 1.5119e-05, 'epoch': 1.89}\n",
            "{'loss': 0.5501, 'learning_rate': 1.5119377994892094e-05, 'epoch': 1.89}\n",
            "01/24/2024 08:23:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.4737, 'learning_rate': 1.4927e-05, 'epoch': 1.90}\n",
            "{'loss': 0.4737, 'learning_rate': 1.4927339107158437e-05, 'epoch': 1.9}\n",
            "01/24/2024 08:23:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.4776, 'learning_rate': 1.4736e-05, 'epoch': 1.90}\n",
            "{'loss': 0.4776, 'learning_rate': 1.4736007153446801e-05, 'epoch': 1.9}\n",
            "01/24/2024 08:24:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.4720, 'learning_rate': 1.4545e-05, 'epoch': 1.91}\n",
            "{'loss': 0.472, 'learning_rate': 1.4545395562092468e-05, 'epoch': 1.91}\n",
            "01/24/2024 08:25:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.6065, 'learning_rate': 1.4356e-05, 'epoch': 1.92}\n",
            "{'loss': 0.6065, 'learning_rate': 1.4355517710873184e-05, 'epoch': 1.92}\n",
            "[INFO|trainer.py:2889] 2024-01-24 08:25:10,237 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1200\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 08:25:10,270 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 08:25:10,270 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1200/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 08:25:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.4855, 'learning_rate': 1.4166e-05, 'epoch': 1.93}\n",
            "{'loss': 0.4855, 'learning_rate': 1.4166386926070322e-05, 'epoch': 1.93}\n",
            "01/24/2024 08:26:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.5650, 'learning_rate': 1.3978e-05, 'epoch': 1.94}\n",
            "{'loss': 0.565, 'learning_rate': 1.397801648153354e-05, 'epoch': 1.94}\n",
            "01/24/2024 08:27:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.5725, 'learning_rate': 1.3790e-05, 'epoch': 1.94}\n",
            "{'loss': 0.5725, 'learning_rate': 1.3790419597749199e-05, 'epoch': 1.94}\n",
            "01/24/2024 08:27:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.5205, 'learning_rate': 1.3604e-05, 'epoch': 1.95}\n",
            "{'loss': 0.5205, 'learning_rate': 1.3603609440912507e-05, 'epoch': 1.95}\n",
            "01/24/2024 08:28:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.3875, 'learning_rate': 1.3418e-05, 'epoch': 1.96}\n",
            "{'loss': 0.3875, 'learning_rate': 1.3417599122003464e-05, 'epoch': 1.96}\n",
            "01/24/2024 08:29:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.5267, 'learning_rate': 1.3232e-05, 'epoch': 1.97}\n",
            "{'loss': 0.5267, 'learning_rate': 1.3232401695866687e-05, 'epoch': 1.97}\n",
            "01/24/2024 08:29:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.6151, 'learning_rate': 1.3048e-05, 'epoch': 1.98}\n",
            "{'loss': 0.6151, 'learning_rate': 1.3048030160295196e-05, 'epoch': 1.98}\n",
            "01/24/2024 08:30:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.5635, 'learning_rate': 1.2864e-05, 'epoch': 1.98}\n",
            "{'loss': 0.5635, 'learning_rate': 1.2864497455118152e-05, 'epoch': 1.98}\n",
            "01/24/2024 08:31:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.4751, 'learning_rate': 1.2682e-05, 'epoch': 1.99}\n",
            "{'loss': 0.4751, 'learning_rate': 1.2681816461292715e-05, 'epoch': 1.99}\n",
            "01/24/2024 08:32:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.4710, 'learning_rate': 1.2500e-05, 'epoch': 2.00}\n",
            "{'loss': 0.471, 'learning_rate': 1.2500000000000006e-05, 'epoch': 2.0}\n",
            "01/24/2024 08:32:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.4994, 'learning_rate': 1.2319e-05, 'epoch': 2.01}\n",
            "{'loss': 0.4994, 'learning_rate': 1.2319060831745272e-05, 'epoch': 2.01}\n",
            "01/24/2024 08:33:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.5386, 'learning_rate': 1.2139e-05, 'epoch': 2.02}\n",
            "{'loss': 0.5386, 'learning_rate': 1.2139011655462337e-05, 'epoch': 2.02}\n",
            "01/24/2024 08:34:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.5548, 'learning_rate': 1.1960e-05, 'epoch': 2.02}\n",
            "{'loss': 0.5548, 'learning_rate': 1.1959865107622307e-05, 'epoch': 2.02}\n",
            "01/24/2024 08:34:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.5555, 'learning_rate': 1.1782e-05, 'epoch': 2.03}\n",
            "{'loss': 0.5555, 'learning_rate': 1.1781633761346707e-05, 'epoch': 2.03}\n",
            "01/24/2024 08:35:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.5451, 'learning_rate': 1.1604e-05, 'epoch': 2.04}\n",
            "{'loss': 0.5451, 'learning_rate': 1.1604330125525079e-05, 'epoch': 2.04}\n",
            "01/24/2024 08:36:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.4848, 'learning_rate': 1.1428e-05, 'epoch': 2.05}\n",
            "{'loss': 0.4848, 'learning_rate': 1.1427966643937069e-05, 'epoch': 2.05}\n",
            "01/24/2024 08:37:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.5546, 'learning_rate': 1.1253e-05, 'epoch': 2.06}\n",
            "{'loss': 0.5546, 'learning_rate': 1.1252555694379006e-05, 'epoch': 2.06}\n",
            "01/24/2024 08:37:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.4971, 'learning_rate': 1.1078e-05, 'epoch': 2.06}\n",
            "{'loss': 0.4971, 'learning_rate': 1.107810958779531e-05, 'epoch': 2.06}\n",
            "01/24/2024 08:38:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.4944, 'learning_rate': 1.0905e-05, 'epoch': 2.07}\n",
            "{'loss': 0.4944, 'learning_rate': 1.0904640567414332e-05, 'epoch': 2.07}\n",
            "01/24/2024 08:39:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.3480, 'learning_rate': 1.0732e-05, 'epoch': 2.08}\n",
            "{'loss': 0.348, 'learning_rate': 1.0732160807889211e-05, 'epoch': 2.08}\n",
            "[INFO|trainer.py:2889] 2024-01-24 08:39:00,882 >> Saving model checkpoint to saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1300\n",
            "[INFO|tokenization_utils_base.py:2432] 2024-01-24 08:39:00,917 >> tokenizer config file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2441] 2024-01-24 08:39:00,918 >> Special tokens file saved in saves/Qwen-1.8B-Chat/lora/train_2024-01-24-05-36-39/tmp-checkpoint-1300/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "01/24/2024 08:39:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.5254, 'learning_rate': 1.0561e-05, 'epoch': 2.09}\n",
            "{'loss': 0.5254, 'learning_rate': 1.0560682414443315e-05, 'epoch': 2.09}\n",
            "01/24/2024 08:40:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.5861, 'learning_rate': 1.0390e-05, 'epoch': 2.10}\n",
            "{'loss': 0.5861, 'learning_rate': 1.03902174220207e-05, 'epoch': 2.1}\n",
            "01/24/2024 08:41:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.4553, 'learning_rate': 1.0221e-05, 'epoch': 2.10}\n",
            "{'loss': 0.4553, 'learning_rate': 1.022077779444145e-05, 'epoch': 2.1}\n",
            "01/24/2024 08:41:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.4791, 'learning_rate': 1.0052e-05, 'epoch': 2.11}\n",
            "{'loss': 0.4791, 'learning_rate': 1.0052375423562038e-05, 'epoch': 2.11}\n",
            "01/24/2024 08:42:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.4957, 'learning_rate': 9.8850e-06, 'epoch': 2.12}\n",
            "{'loss': 0.4957, 'learning_rate': 9.88502212844063e-06, 'epoch': 2.12}\n",
            "01/24/2024 08:43:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.5127, 'learning_rate': 9.7187e-06, 'epoch': 2.13}\n",
            "{'loss': 0.5127, 'learning_rate': 9.718729654507713e-06, 'epoch': 2.13}\n",
            "01/24/2024 08:43:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.5490, 'learning_rate': 9.5535e-06, 'epoch': 2.14}\n",
            "{'loss': 0.549, 'learning_rate': 9.553509672741645e-06, 'epoch': 2.14}\n",
            "01/24/2024 08:44:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.5425, 'learning_rate': 9.3894e-06, 'epoch': 2.14}\n",
            "{'loss': 0.5425, 'learning_rate': 9.389373778849612e-06, 'epoch': 2.14}\n",
            "01/24/2024 08:45:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.4129, 'learning_rate': 9.2263e-06, 'epoch': 2.15}\n",
            "{'loss': 0.4129, 'learning_rate': 9.22633349245376e-06, 'epoch': 2.15}\n",
            "01/24/2024 08:45:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.5671, 'learning_rate': 9.0644e-06, 'epoch': 2.16}\n",
            "{'loss': 0.5671, 'learning_rate': 9.064400256282757e-06, 'epoch': 2.16}\n",
            "01/24/2024 08:46:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.5407, 'learning_rate': 8.9036e-06, 'epoch': 2.17}\n",
            "{'loss': 0.5407, 'learning_rate': 8.903585435368658e-06, 'epoch': 2.17}\n",
            "01/24/2024 08:47:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.4707, 'learning_rate': 8.7439e-06, 'epoch': 2.18}\n",
            "{'loss': 0.4707, 'learning_rate': 8.743900316249273e-06, 'epoch': 2.18}\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 2 開啟WebUI\n",
        "#@markdown #STEP 2\n",
        "#@markdown ##開啟WebUI\n",
        "#@markdown ##Run the WebUI\n",
        "%cd /content/LLaMA-Factory\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Qwen-1_8B-Chat\n",
        "! ls"
      ],
      "metadata": {
        "id": "S8prYbgC-_B9",
        "outputId": "fa840abc-2596-4e5b-ab75-865328b2630e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Qwen-1_8B-Chat\n",
            "assets\t\t\t\t   generation_config.json\t     qwen_generation_utils.py\n",
            "cache_autogptq_cuda_256.cpp\t   LICENSE\t\t\t     qwen.tiktoken\n",
            "cache_autogptq_cuda_kernel_256.cu  model-00001-of-00002.safetensors  README.md\n",
            "config.json\t\t\t   model-00002-of-00002.safetensors  tokenization_qwen.py\n",
            "configuration_qwen.py\t\t   modeling_qwen.py\t\t     tokenizer_config.json\n",
            "cpp_kernels.py\t\t\t   model.safetensors.index.json\n",
            "examples\t\t\t   NOTICE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip list"
      ],
      "metadata": {
        "id": "fwb_eSVi7AcJ",
        "outputId": "0dde0eb7-63f2-46e7-d6a7-62f7f66138f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- ---------------------\n",
            "absl-py                          1.4.0\n",
            "accelerate                       0.26.1\n",
            "aiofiles                         23.2.1\n",
            "aiohttp                          3.9.1\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.16\n",
            "albumentations                   1.3.1\n",
            "altair                           4.2.2\n",
            "anyio                            3.7.1\n",
            "appdirs                          1.4.4\n",
            "argon2-cffi                      23.1.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array-record                     0.5.0\n",
            "arviz                            0.15.1\n",
            "astropy                          5.3.4\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.3\n",
            "atpublic                         4.0\n",
            "attrs                            23.2.0\n",
            "audioread                        3.0.1\n",
            "autograd                         1.6.2\n",
            "Babel                            2.14.0\n",
            "backcall                         0.2.0\n",
            "backoff                          2.2.1\n",
            "beautifulsoup4                   4.11.2\n",
            "bidict                           0.22.1\n",
            "bigframes                        0.19.1\n",
            "bitsandbytes                     0.42.0\n",
            "bleach                           6.1.0\n",
            "blinker                          1.4\n",
            "blis                             0.7.11\n",
            "blosc2                           2.0.0\n",
            "bokeh                            3.3.3\n",
            "bqplot                           0.12.42\n",
            "branca                           0.7.0\n",
            "build                            1.0.3\n",
            "CacheControl                     0.13.1\n",
            "cachetools                       5.3.2\n",
            "catalogue                        2.0.10\n",
            "certifi                          2023.11.17\n",
            "cffi                             1.16.0\n",
            "chardet                          5.2.0\n",
            "charset-normalizer               3.3.2\n",
            "chex                             0.1.7\n",
            "click                            8.1.7\n",
            "click-plugins                    1.1.1\n",
            "cligj                            0.7.2\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.27.9\n",
            "cmdstanpy                        1.2.0\n",
            "cohere                           4.44\n",
            "colorcet                         3.0.1\n",
            "colorlover                       0.3.0\n",
            "colour                           0.1.5\n",
            "community                        1.0.0b1\n",
            "confection                       0.1.4\n",
            "cons                             0.4.6\n",
            "contextlib2                      21.6.0\n",
            "contourpy                        1.2.0\n",
            "cryptography                     41.0.7\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda12x                     12.2.0\n",
            "cvxopt                           1.3.2\n",
            "cvxpy                            1.3.2\n",
            "cycler                           0.12.1\n",
            "cymem                            2.0.8\n",
            "Cython                           3.0.8\n",
            "dask                             2023.8.1\n",
            "datascience                      0.17.6\n",
            "datasets                         2.16.1\n",
            "db-dtypes                        1.2.0\n",
            "dbus-python                      1.2.18\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "dill                             0.3.7\n",
            "diskcache                        5.6.3\n",
            "distributed                      2023.8.1\n",
            "distro                           1.7.0\n",
            "dlib                             19.24.2\n",
            "dm-tree                          0.1.8\n",
            "docstring-parser                 0.15\n",
            "docutils                         0.18.1\n",
            "dopamine-rl                      4.0.6\n",
            "duckdb                           0.9.2\n",
            "earthengine-api                  0.1.386\n",
            "easydict                         1.11\n",
            "ecos                             2.0.12\n",
            "editdistance                     0.6.2\n",
            "eerepr                           0.0.4\n",
            "einops                           0.7.0\n",
            "en-core-web-sm                   3.6.0\n",
            "entrypoints                      0.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.6.0\n",
            "etuples                          0.3.9\n",
            "exceptiongroup                   1.2.0\n",
            "fastai                           2.7.13\n",
            "fastapi                          0.109.0\n",
            "fastavro                         1.9.3\n",
            "fastcore                         1.5.29\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.19.1\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.2\n",
            "ffmpy                            0.3.1\n",
            "filelock                         3.13.1\n",
            "fiona                            1.9.5\n",
            "firebase-admin                   5.3.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      23.5.26\n",
            "flax                             0.7.5\n",
            "folium                           0.14.0\n",
            "fonttools                        4.47.2\n",
            "frozendict                       2.4.0\n",
            "frozenlist                       1.4.1\n",
            "fsspec                           2023.6.0\n",
            "future                           0.18.3\n",
            "gast                             0.5.4\n",
            "gcsfs                            2023.6.0\n",
            "GDAL                             3.4.3\n",
            "gdown                            4.7.3\n",
            "geemap                           0.30.4\n",
            "gensim                           4.3.2\n",
            "geocoder                         1.38.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        0.13.2\n",
            "geopy                            2.3.0\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-ai-generativelanguage     0.4.0\n",
            "google-api-core                  2.11.1\n",
            "google-api-python-client         2.84.0\n",
            "google-auth                      2.17.3\n",
            "google-auth-httplib2             0.1.1\n",
            "google-auth-oauthlib             1.2.0\n",
            "google-cloud-aiplatform          1.39.0\n",
            "google-cloud-bigquery            3.12.0\n",
            "google-cloud-bigquery-connection 1.12.1\n",
            "google-cloud-bigquery-storage    2.24.0\n",
            "google-cloud-core                2.3.3\n",
            "google-cloud-datastore           2.15.2\n",
            "google-cloud-firestore           2.11.1\n",
            "google-cloud-functions           1.13.3\n",
            "google-cloud-iam                 2.13.0\n",
            "google-cloud-language            2.9.1\n",
            "google-cloud-resource-manager    1.11.0\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.11.3\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.5.0\n",
            "google-generativeai              0.3.2\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.7.0\n",
            "googleapis-common-protos         1.62.0\n",
            "googledrivedownloader            0.4\n",
            "gradio                           3.50.2\n",
            "gradio_client                    0.6.1\n",
            "graphviz                         0.20.1\n",
            "greenlet                         3.0.3\n",
            "grpc-google-iam-v1               0.13.0\n",
            "grpcio                           1.60.0\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          3.4.2\n",
            "gspread-dataframe                3.3.1\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h11                              0.14.0\n",
            "h5netcdf                         1.3.0\n",
            "h5py                             3.9.0\n",
            "holidays                         0.41\n",
            "holoviews                        1.17.1\n",
            "html5lib                         1.1\n",
            "httpcore                         1.0.2\n",
            "httpimport                       1.3.1\n",
            "httplib2                         0.22.0\n",
            "httpx                            0.26.0\n",
            "huggingface-hub                  0.20.2\n",
            "humanize                         4.7.0\n",
            "hyperopt                         0.2.7\n",
            "ibis-framework                   7.1.0\n",
            "idna                             3.6\n",
            "imageio                          2.31.6\n",
            "imageio-ffmpeg                   0.4.9\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.10.1\n",
            "imgaug                           0.4.0\n",
            "importlib-metadata               6.11.0\n",
            "importlib-resources              6.1.1\n",
            "imutils                          0.5.4\n",
            "inflect                          7.0.0\n",
            "iniconfig                        2.0.0\n",
            "install                          1.3.5\n",
            "intel-openmp                     2023.2.3\n",
            "ipyevents                        2.0.2\n",
            "ipyfilechooser                   0.6.0\n",
            "ipykernel                        5.5.6\n",
            "ipyleaflet                       0.18.1\n",
            "ipython                          7.34.0\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.5.0\n",
            "ipytree                          0.2.2\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.1.2\n",
            "jax                              0.4.23\n",
            "jaxlib                           0.4.23+cuda12.cudnn89\n",
            "jeepney                          0.7.1\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.3\n",
            "joblib                           1.3.2\n",
            "jsonpickle                       3.0.2\n",
            "jsonschema                       4.19.2\n",
            "jsonschema-specifications        2023.12.1\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.7.1\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab_pygments              0.3.0\n",
            "jupyterlab-widgets               3.0.9\n",
            "kaggle                           1.5.16\n",
            "kagglehub                        0.1.5\n",
            "kaleido                          0.2.1\n",
            "keras                            2.15.0\n",
            "keyring                          23.5.0\n",
            "kiwisolver                       1.4.5\n",
            "langcodes                        3.3.0\n",
            "launchpadlib                     1.10.16\n",
            "lazr.restfulclient               0.14.4\n",
            "lazr.uri                         1.0.6\n",
            "lazy_loader                      0.3\n",
            "libclang                         16.0.6\n",
            "librosa                          0.10.1\n",
            "lida                             0.0.10\n",
            "lightgbm                         4.1.0\n",
            "linkify-it-py                    2.0.2\n",
            "llmx                             0.0.15a0\n",
            "llvmlite                         0.41.1\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "lxml                             4.9.4\n",
            "malloy                           2023.1067\n",
            "Markdown                         3.5.2\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.3\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.6\n",
            "matplotlib-venn                  0.11.9\n",
            "mdit-py-plugins                  0.4.0\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          0.8.4\n",
            "mizani                           0.9.3\n",
            "mkl                              2023.2.0\n",
            "ml-dtypes                        0.2.0\n",
            "mlxtend                          0.22.0\n",
            "more-itertools                   10.1.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.7\n",
            "multidict                        6.0.4\n",
            "multipledispatch                 1.0.0\n",
            "multiprocess                     0.70.15\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.10\n",
            "music21                          9.1.0\n",
            "natsort                          8.4.0\n",
            "nbclassic                        1.0.0\n",
            "nbclient                         0.9.0\n",
            "nbconvert                        6.5.4\n",
            "nbformat                         5.9.2\n",
            "nest-asyncio                     1.5.9\n",
            "networkx                         3.2.1\n",
            "nibabel                          4.0.2\n",
            "nltk                             3.8.1\n",
            "notebook                         6.5.5\n",
            "notebook_shim                    0.2.3\n",
            "numba                            0.58.1\n",
            "numexpr                          2.8.8\n",
            "numpy                            1.23.5\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "openai                           1.9.0\n",
            "opencv-contrib-python            4.8.0.76\n",
            "opencv-python                    4.8.0.76\n",
            "opencv-python-headless           4.9.0.80\n",
            "openpyxl                         3.1.2\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.1.8\n",
            "orbax-checkpoint                 0.4.4\n",
            "orjson                           3.9.12\n",
            "osqp                             0.6.2.post8\n",
            "packaging                        23.2\n",
            "pandas                           1.5.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.19.2\n",
            "pandas-stubs                     1.5.3.230304\n",
            "pandocfilters                    1.5.0\n",
            "panel                            1.3.6\n",
            "param                            2.0.2\n",
            "parso                            0.8.3\n",
            "parsy                            2.1\n",
            "partd                            1.4.1\n",
            "pathlib                          1.0.1\n",
            "pathlib_abc                      0.1.1\n",
            "pathy                            0.11.0\n",
            "patsy                            0.5.6\n",
            "peewee                           3.17.0\n",
            "peft                             0.7.1\n",
            "pexpect                          4.9.0\n",
            "pickleshare                      0.7.5\n",
            "Pillow                           9.4.0\n",
            "pins                             0.8.4\n",
            "pip                              23.1.2\n",
            "pip-tools                        6.13.0\n",
            "platformdirs                     4.1.0\n",
            "plotly                           5.15.0\n",
            "plotnine                         0.12.4\n",
            "pluggy                           1.3.0\n",
            "polars                           0.20.2\n",
            "pooch                            1.8.0\n",
            "portpicker                       1.5.2\n",
            "prefetch-generator               1.0.3\n",
            "preshed                          3.0.9\n",
            "prettytable                      3.9.0\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.2.0\n",
            "prometheus-client                0.19.0\n",
            "promise                          2.3\n",
            "prompt-toolkit                   3.0.43\n",
            "prophet                          1.1.5\n",
            "proto-plus                       1.23.0\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.9\n",
            "ptyprocess                       0.7.0\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          10.0.1\n",
            "pyarrow-hotfix                   0.6\n",
            "pyasn1                           0.5.1\n",
            "pyasn1-modules                   0.3.0\n",
            "pycocotools                      2.0.7\n",
            "pycparser                        2.21\n",
            "pyct                             0.5.0\n",
            "pydantic                         1.10.13\n",
            "pydata-google-auth               1.8.2\n",
            "pydot                            1.4.2\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "PyDrive2                         1.6.3\n",
            "pydub                            0.25.1\n",
            "pyerfa                           2.0.1.1\n",
            "pygame                           2.5.2\n",
            "Pygments                         2.16.1\n",
            "PyGObject                        3.42.1\n",
            "PyJWT                            2.3.0\n",
            "pymc                             5.7.2\n",
            "pymystem3                        0.2.0\n",
            "PyOpenGL                         3.1.7\n",
            "pyOpenSSL                        23.3.0\n",
            "pyparsing                        3.1.1\n",
            "pyperclip                        1.8.2\n",
            "pyproj                           3.6.1\n",
            "pyproject_hooks                  1.0.0\n",
            "pyshp                            2.3.1\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.14.2\n",
            "pytest                           7.4.4\n",
            "python-apt                       0.0.0\n",
            "python-box                       7.1.1\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-multipart                 0.0.6\n",
            "python-slugify                   8.0.1\n",
            "python-utils                     3.8.1\n",
            "pytz                             2023.3.post1\n",
            "pyviz_comms                      3.0.1\n",
            "PyWavelets                       1.5.0\n",
            "PyYAML                           6.0.1\n",
            "pyzmq                            23.2.1\n",
            "qdldl                            0.1.7.post0\n",
            "qudida                           0.0.4\n",
            "ratelim                          0.1.6\n",
            "referencing                      0.32.1\n",
            "regex                            2023.6.3\n",
            "requests                         2.31.0\n",
            "requests-oauthlib                1.3.1\n",
            "requirements-parser              0.5.0\n",
            "rich                             13.7.0\n",
            "rouge-chinese                    1.0.3\n",
            "rpds-py                          0.17.1\n",
            "rpy2                             3.4.2\n",
            "rsa                              4.9\n",
            "safetensors                      0.4.1\n",
            "scikit-image                     0.19.3\n",
            "scikit-learn                     1.2.2\n",
            "scipy                            1.11.4\n",
            "scooby                           0.9.2\n",
            "scs                              3.2.4.post1\n",
            "seaborn                          0.13.1\n",
            "SecretStorage                    3.3.1\n",
            "semantic-version                 2.10.0\n",
            "Send2Trash                       1.8.2\n",
            "sentencepiece                    0.1.99\n",
            "setuptools                       67.7.2\n",
            "shapely                          2.0.2\n",
            "shtab                            1.6.5\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       6.4.0\n",
            "sniffio                          1.3.0\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "soundfile                        0.12.1\n",
            "soupsieve                        2.5\n",
            "soxr                             0.3.7\n",
            "spacy                            3.6.1\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.5\n",
            "Sphinx                           5.0.2\n",
            "sphinxcontrib-applehelp          1.0.8\n",
            "sphinxcontrib-devhelp            1.0.6\n",
            "sphinxcontrib-htmlhelp           2.0.5\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             1.0.7\n",
            "sphinxcontrib-serializinghtml    1.1.10\n",
            "SQLAlchemy                       2.0.24\n",
            "sqlglot                          19.9.0\n",
            "sqlparse                         0.4.4\n",
            "srsly                            2.4.8\n",
            "sse-starlette                    1.8.2\n",
            "stanio                           0.3.0\n",
            "starlette                        0.35.1\n",
            "statsmodels                      0.14.1\n",
            "sympy                            1.12\n",
            "tables                           3.8.0\n",
            "tabulate                         0.9.0\n",
            "tbb                              2021.11.0\n",
            "tblib                            3.0.0\n",
            "tenacity                         8.2.3\n",
            "tensorboard                      2.15.1\n",
            "tensorboard-data-server          0.7.2\n",
            "tensorflow                       2.15.0\n",
            "tensorflow-datasets              4.9.4\n",
            "tensorflow-estimator             2.15.0\n",
            "tensorflow-gcs-config            2.15.0\n",
            "tensorflow-hub                   0.15.0\n",
            "tensorflow-io-gcs-filesystem     0.35.0\n",
            "tensorflow-metadata              1.14.0\n",
            "tensorflow-probability           0.22.0\n",
            "tensorstore                      0.1.45\n",
            "termcolor                        2.4.0\n",
            "terminado                        0.18.0\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.1.12\n",
            "threadpoolctl                    3.2.0\n",
            "tifffile                         2023.12.9\n",
            "tiktoken                         0.5.2\n",
            "tinycss2                         1.2.1\n",
            "tokenizers                       0.15.0\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.0\n",
            "torch                            2.1.0+cu121\n",
            "torchaudio                       2.1.0+cu121\n",
            "torchdata                        0.7.0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.16.0\n",
            "torchvision                      0.16.0+cu121\n",
            "tornado                          6.3.2\n",
            "tqdm                             4.66.1\n",
            "traitlets                        5.7.1\n",
            "traittypes                       0.2.1\n",
            "transformers                     4.37.0\n",
            "transformers-stream-generator    0.0.4\n",
            "triton                           2.1.0\n",
            "trl                              0.7.10\n",
            "tweepy                           4.14.0\n",
            "typer                            0.9.0\n",
            "types-pytz                       2023.3.1.1\n",
            "types-setuptools                 69.0.0.20240115\n",
            "typing_extensions                4.9.0\n",
            "tyro                             0.6.6\n",
            "tzlocal                          5.2\n",
            "uc-micro-py                      1.0.2\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          2.0.7\n",
            "uvicorn                          0.27.0\n",
            "vega-datasets                    0.9.0\n",
            "wadllib                          1.3.6\n",
            "wasabi                           1.1.2\n",
            "wcwidth                          0.2.13\n",
            "webcolors                        1.13\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.7.0\n",
            "websockets                       11.0.3\n",
            "Werkzeug                         3.0.1\n",
            "wheel                            0.42.0\n",
            "widgetsnbextension               3.6.6\n",
            "wordcloud                        1.9.3\n",
            "wrapt                            1.14.1\n",
            "xarray                           2023.7.0\n",
            "xarray-einstats                  0.7.0\n",
            "xgboost                          2.0.3\n",
            "xlrd                             2.0.1\n",
            "xxhash                           3.4.1\n",
            "xyzservices                      2023.10.1\n",
            "yarl                             1.9.4\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.35\n",
            "zict                             3.0.0\n",
            "zipp                             3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwzJxqQp6euI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I11rZlhctzd9"
      },
      "source": [
        "#STEP 3 上傳輸出的檔案到 HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rte6fcaejOsC"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.1 安裝HuggingFace API\n",
        "#@markdown #STEP 3.1\n",
        "#@markdown ##為了上傳模型至HuggingFace，需要先安裝HF的函式庫\n",
        "#@markdown ##In order to upload the model to HuggingFace, we have to install the HF library first.\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRbS8HBWTZpu"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.2 使用API Token登入\n",
        "#@markdown #STEP 3.2\n",
        "#@markdown ##執行本儲存格後填入 API Token(需要有 Write 權限)，然後按下登入\n",
        "#@markdown ##After runing this cell, fill in the API Token (Write permission is required), and then click Login\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd7sv-ZvT4KB"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3.3 上傳模型\n",
        "#@markdown #STEP 3.3\n",
        "#@markdown ##執行本儲存格後填入 API Token(需要有 Write 權限)，然後按下登入\n",
        "#@markdown ##After runing this cell, fill in the API Token (Write permission is required), and then click Login\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"username/fine-tune-model\" #@param {type:\"string\"}\n",
        "export_folder_path = \"/content/LLaMA-Factory/export\" #@param {type:\"string\"}\n",
        "api.create_repo(model_id, private=True, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_folder(\n",
        "    folder_path=export_folder_path,\n",
        "    repo_id=model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7VH8k-s0G71"
      },
      "source": [
        "#STEP 4 保存到Google Drive(自選)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2StL2Wqt0MAt"
      },
      "outputs": [],
      "source": [
        "#@title STEP 4.1 連接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72nLh_L50g74"
      },
      "outputs": [],
      "source": [
        "#@title STEP 4.2 複製檔案到Google Drive\n",
        "Google_Drive_Folder = \"/content/drive/MyDrive/LLaMA-Factory/\" #@param {type:\"string\"}\n",
        "\n",
        "!cp -rf /content/LLaMA-Factory/saves {Google_Drive_Folder}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}