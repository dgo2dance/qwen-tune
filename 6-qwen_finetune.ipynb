{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO43vgU3VXP/FqT97QsY29F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/qwen_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdJeKM2_c3lF"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-50AbTRLCh5D"
      },
      "outputs": [],
      "source": [
        "\"\"\"下载代码\n",
        "\"\"\"\n",
        "%cd /content/\n",
        "! rm -rf  /content/qwen-finetune-demo/\n",
        "!git clone https://gitee.com/duanshuyong/qwen-finetune-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/\n",
        "!ls\n",
        "!pip install -U -r requirements.txt"
      ],
      "metadata": {
        "id": "0684LhvlfEbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/data/\n",
        "!ls\n",
        "!mkdir -p nlp/pre_models/torch/qwen\n",
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/data/nlp/pre_models/torch/qwen\n",
        "!git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B\n",
        "!git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat\n"
      ],
      "metadata": {
        "id": "7CQ7dtHcej-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_lora.sh -m dataset"
      ],
      "metadata": {
        "id": "jbTIH3EoQX2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/infer/\n",
        " !python infer.py"
      ],
      "metadata": {
        "id": "Tbhi6vEPi8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_lora.sh -m dataset"
      ],
      "metadata": {
        "id": "PrMv62uOjchk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_lora.sh -m train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnHyQcufjuy8",
        "outputId": "b9da9930-98e7-4f1d-e4dd-6051919c43d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/scripts\n",
            "2024-01-21 02:47:01.621862: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-21 02:47:01.621909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-21 02:47:01.623510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-21 02:47:02.906803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-21 02:47:06,618] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "INFO:root:make_dataset ../data/*.json train...\n",
            "INFO:root:make data ./outputs_pl/dataset_file_0_dupe_factor_0-train.parquet...\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:558: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "INFO:lightning_fabric.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning_fabric.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:lightning_fabric.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning_fabric.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:lightning_fabric.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "QWenConfig {\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"fp16\": true,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"return_dict\": false,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"learning_rate_for_task\": 2e-05\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "TrainingArguments(optimizer='lion', optimizer_args=None, scheduler_type='CAWR', scheduler={'T_mult': 1, 'rewarm_epoch_num': 0.5, 'verbose': False}, adv=None, hierarchical_position=None, learning_rate=2e-05, learning_rate_for_task=2e-05, max_epochs=20, max_steps=-1, optimizer_betas=(0.9, 0.999), adam_epsilon=1e-08, gradient_accumulation_steps=1, max_grad_norm=1.0, weight_decay=0, warmup_steps=0, train_batch_size=2, eval_batch_size=2, test_batch_size=2, seed=42, dataloader_drop_last=True, dataloader_num_workers=0, dataloader_pin_memory=True, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None)\n",
            "ModelArguments(model_name_or_path='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', model_type='qwen', config_overrides=None, config_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', tokenizer_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', processer_name=None, imageprocesser_name=None, feature_extractor_name=None, cache_dir=None, do_lower_case=None, use_fast_tokenizer=False, model_revision='main', use_auth_token=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, model_custom={})\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:13<00:00,  7.00s/it]\n",
            "============================================================ lora info\n",
            "trainable params: 1,572,864 || all params: 1,838,401,536 || trainable%: 0.08555606428736143\n",
            "****************************** total 500\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "WARNING: Missing logger folder: outputs_pl/lightning_logs\n",
            "WARNING:lightning.pytorch.loggers.tensorboard:Missing logger folder: outputs_pl/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/best_ckpt exists and is not empty.\n",
            "INFO:lightning_fabric.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name             | Type      | Params\n",
            "-----------------------------------------------\n",
            "0 | transformer_base | PetlModel | 1.8 B \n",
            "-----------------------------------------------\n",
            "1.6 M     Trainable params\n",
            "1.8 B     Non-trainable params\n",
            "1.8 B     Total params\n",
            "7,353.606 Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name             | Type      | Params\n",
            "-----------------------------------------------\n",
            "0 | transformer_base | PetlModel | 1.8 B \n",
            "-----------------------------------------------\n",
            "1.6 M     Trainable params\n",
            "1.8 B     Non-trainable params\n",
            "1.8 B     Total params\n",
            "7,353.606 Total estimated model params size (MB)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "Epoch 19: 100% 250/250 [01:20<00:00,  3.09it/s, v_num=0, loss=9.85e-5] INFO:lightning_fabric.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "Epoch 19: 100% 250/250 [01:20<00:00,  3.09it/s, v_num=0, loss=9.85e-5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hvc7g5iMRZ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/infer/\n",
        " !python infer_lora_finetuning.py"
      ],
      "metadata": {
        "id": "CAo3L8zWr6MP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3db56a5-2d19-4454-d9c0-77542d268544"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/infer\n",
            "2024-01-21 03:18:38.336531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-21 03:18:38.336580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-21 03:18:38.338131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-21 03:18:39.803425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "QWenConfig {\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"fp16\": true,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"return_dict\": false,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"learning_rate_for_task\": 2e-05\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "None\n",
            "ModelArguments(model_name_or_path='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', model_type='qwen', config_overrides=None, config_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', tokenizer_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B-Chat', processer_name=None, imageprocesser_name=None, feature_extractor_name=None, cache_dir=None, do_lower_case=None, use_fast_tokenizer=False, model_revision='main', use_auth_token=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, model_custom={})\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  2.00s/it]\n",
            "============================================================ lora info\n",
            "trainable params: 0 || all params: 1,838,401,536 || trainable%: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/qwen-finetune-demo/qwen_finetuning-dev/scripts/best_ckpt/last/pytorch_model_merge.bin/model.safetensors\")"
      ],
      "metadata": {
        "id": "p5k6I2O_D6WU",
        "outputId": "33b4406f-9c81-4fff-95cd-a51cceeeb650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1de8cfcb-0b9c-450a-a7ee-4053ed3a1153\", \"model.safetensors\", 3673678216)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "fhUvLXgjHm_Z",
        "outputId": "cab724b9-c293-4ac9-d657-67bff888622f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/best_ckpt/last/pytorch_model_merge.bin/model.safetensors   \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "id": "YEqi2aTnJT64"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
