{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/QWEN_LORA_GGUF_Converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "python3 --version\n",
        "make --version\n",
        "g++ --version"
      ],
      "metadata": {
        "id": "ysh3Cx9hSGY6",
        "outputId": "b4063cb8-505d-432e-bfc5-8334b9f77e7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "GNU Make 4.3\n",
            "Built for x86_64-pc-linux-gnu\n",
            "Copyright (C) 1988-2020 Free Software Foundation, Inc.\n",
            "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
            "This is free software: you are free to change and redistribute it.\n",
            "There is NO WARRANTY, to the extent permitted by law.\n",
            "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bb003dHqcQdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0e2ba0-c014-45df-d920-2c4c630d916a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 17153, done.\u001b[K\n",
            "remote: Counting objects: 100% (6153/6153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (542/542), done.\u001b[K\n",
            "remote: Total 17153 (delta 5879), reused 5766 (delta 5610), pack-reused 11000\u001b[K\n",
            "Receiving objects: 100% (17153/17153), 19.21 MiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (11971/11971), done.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/imatrix/imatrix.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o imatrix  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece~=0.1.98 (from -r ./requirements/requirements-convert.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: sentencepiece, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.2 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.2 sentencepiece-0.1.99 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "! rm -rf  /content/llama.cpp/\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp && make && pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B\n",
        "!git clone https://huggingface.co/shuyong/qwen-1_8-fine-tune-model-loss\n",
        "\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkodq6Z4OCOy",
        "outputId": "4e0dfadd-4316-4b0b-a68c-409600471295"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'qwen-1_8-fine-tune-model-loss'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (18/18), 1.11 MiB | 3.14 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.42 GiB | 47.66 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ENwsZ0_fYMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9569d10-86b2-44d0-cd20-023489296beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-transfer\n",
            "  Downloading hf_transfer-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-transfer\n",
            "Successfully installed hf-transfer-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install hf-transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66mMgKSfbbw",
        "outputId": "f51cf5e0-d48a-4f81-d235-34bd35f5a996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/.gitattributes to /root/.cache/huggingface/hub/tmpiyvj13ao\n",
            "\r.gitattributes:   0% 0.00/1.52k [00:00<?, ?B/s]\r.gitattributes: 100% 1.52k/1.52k [00:00<00:00, 3.73MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/README.md to /root/.cache/huggingface/hub/tmp3q00pxti\n",
            "\rREADME.md:   0% 0.00/8.73k [00:00<?, ?B/s]\rREADME.md: 100% 8.73k/8.73k [00:00<00:00, 24.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/Research%20License.docx to /root/.cache/huggingface/hub/tmppc3ttgr8\n",
            "Research License.docx: 100% 38.9k/38.9k [00:00<00:00, 29.6MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/added_tokens.json to /root/.cache/huggingface/hub/tmp04i_95c4\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 4.41MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/config.json to /root/.cache/huggingface/hub/tmpdixevjjl\n",
            "config.json: 100% 732/732 [00:00<00:00, 2.76MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/configuration_phi.py to /root/.cache/huggingface/hub/tmpalphsppd\n",
            "configuration_phi.py: 100% 2.03k/2.03k [00:00<00:00, 6.01MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/generation_config.json to /root/.cache/huggingface/hub/tmp2oajp464\n",
            "generation_config.json: 100% 69.0/69.0 [00:00<00:00, 245kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/merges.txt to /root/.cache/huggingface/hub/tmpodezm9of\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 20.2MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/modeling_phi.py to /root/.cache/huggingface/hub/tmp1aus7yci\n",
            "modeling_phi.py: 100% 33.5k/33.5k [00:00<00:00, 63.7MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/pytorch_model.bin to /root/.cache/huggingface/hub/tmpimgts1rq\n",
            "pytorch_model.bin: 100% 2.84G/2.84G [00:20<00:00, 136MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/special_tokens_map.json to /root/.cache/huggingface/hub/tmp_cic1_yz\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 405kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer.json to /root/.cache/huggingface/hub/tmpr1dlp5fq\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 27.8MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer_config.json to /root/.cache/huggingface/hub/tmpvazjcj8u\n",
            "tokenizer_config.json: 100% 237/237 [00:00<00:00, 993kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/vocab.json to /root/.cache/huggingface/hub/tmpi0gu_jjc\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 50.4MB/s]\n",
            "/content/phi-1_5\n"
          ]
        }
      ],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli download microsoft/phi-1_5 --local-dir phi-1_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drRiT7wPhgA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738395c1-c037-4cd6-e507-e193e9754661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: phi-1_5\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 50000 merge(s).\n",
            "gguf: Setting special token type bos to 50256\n",
            "gguf: Setting special token type eos to 50256\n",
            "gguf: Setting special token type unk to 50256\n",
            "Exporting model to 'phi-1_5/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'pytorch_model.bin'\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "token_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "output.weight, n_dims = 2, torch.float16 --> float16\n",
            "output.bias, n_dims = 1, torch.float16 --> float32\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 1173, in <module>\n",
            "    model_instance.write()\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 139, in write\n",
            "    self.gguf_writer.write_tensors_to_file()\n",
            "  File \"/content/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 264, in write_tensors_to_file\n",
            "    tensor.tofile(self.fout)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 llama.cpp/convert-hf-to-gguf.py ./phi-1_5/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHwHI-d1Ojv1",
        "outputId": "cd0ee4e9-3aa6-45ac-a3b7-cd9915ccede5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 llama.cpp/convert-hf-to-gguf.py ./qwen-1_8-fine-tune-model-loss/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCL8hb-XOVCu",
        "outputId": "8a316afd-a53d-4334-def0-7b36607fddce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: qwen-1_8-fine-tune-model-loss\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 151387 merge(s).\n",
            "gguf: Setting special token type bos to 151643\n",
            "gguf: Setting special token type eos to 151643\n",
            "gguf: Setting special token type unk to 151643\n",
            "Exporting model to 'qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "token_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_gate.weight, n_dims = 2, torch.float16 --> float16\n",
            "output_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "Model successfully exported to 'qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd llama.cpp\n",
        "git log | head -1\n",
        "python3 --version\n",
        "pip list | egrep \"torch|numpy|sentencepiece\"\n",
        "make --version | head -1\n",
        "\n",
        "md5sum ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
        "md5sum ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvv4fmVkUEC0",
        "outputId": "94c6b865-df9d-4ca6-a253-706359a8f891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commit 708e179e8562c2604240df95a2241dea17fd808b\n",
            "Python 3.10.12\n",
            "numpy                            1.24.4\n",
            "sentencepiece                    0.1.98\n",
            "torch                            2.1.0+cu121\n",
            "torchaudio                       2.1.0+cu121\n",
            "torchdata                        0.7.0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.16.0\n",
            "torchvision                      0.16.0+cu121\n",
            "GNU Make 4.3\n",
            "83af84a363c8593a8ed930b4033c4868  ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
            "706ead5763dbc23b9a1e1388ce7721ee  ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMeEnRZpcfoa"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q8_0\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q6_K\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q5_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q4_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q3_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q2_K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd dolphin-2_6-phi-2 && rename 's/^ggml-model-/dolphin-2_6-phi-2-/' ggml-model-*"
      ],
      "metadata": {
        "id": "vsKWnTLw5FV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli upload dolphin-2_6-phi-2-GGUF ./dolphin-2_6-phi-2/ ."
      ],
      "metadata": {
        "id": "JEIYZV9Z5OC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX-sZuxxrkYV"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/main -m ./dolphin-2_6-phi-2/ggml-model-Q4_K_M.gguf -p \"What's up?\" -n 400 -e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !llama.cpp/main -m Qwen-1_8B-Chat/ggml-model-f16.gguf -n 512 -p \"你叫什么\"  --chatml\n",
        "# !llama.cpp/main -m Qwen-1_8B-Chat/ggml-model-f16.gguf --lora ggml-adapter-model.bin -n 512 -p \"你是一个人工智能助手\" --chatml\n",
        "!llama.cpp/main -m qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf -n 512 -p \"你是一个人工智能助手\" --chatml"
      ],
      "metadata": {
        "id": "-JYiIzMEO8th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llama.cpp/quantize qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf Q5_K_M\n"
      ],
      "metadata": {
        "id": "iFFAnYS2cSVk",
        "outputId": "a28c9b8b-2941-4324-faa6-6101a5684a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1983 (a1d6df12)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf' to 'qwen-1_8-fine-tune-model-loss/ggml-model-Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 17 key-value pairs and 195 tensors from qwen-1_8-fine-tune-model-loss/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
            "llama_model_loader: - kv   2:                        qwen.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                           qwen.block_count u32              = 24\n",
            "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
            "llama_model_loader: - type  f32:   73 tensors\n",
            "llama_model_loader: - type  f16:  122 tensors\n",
            "llama_model_quantize_internal: meta size = 5940032 bytes\n",
            "[   1/ 195]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[   2/ 195]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[   3/ 195]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   4/ 195]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   5/ 195]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 195]                blk.0.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[   7/ 195]                  blk.0.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[   8/ 195]                blk.0.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[   9/ 195]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  10/ 195]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  11/ 195]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  12/ 195]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 195]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 195]                blk.1.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  15/ 195]                  blk.1.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  16/ 195]                blk.1.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  17/ 195]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  18/ 195]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  19/ 195]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  20/ 195]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  21/ 195]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  22/ 195]               blk.10.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.019 0.030 0.047 0.066 0.088 0.108 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[  23/ 195]                 blk.10.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  24/ 195]               blk.10.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  25/ 195]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  26/ 195]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  27/ 195]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  28/ 195]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  29/ 195]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  30/ 195]               blk.11.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.037 0.026 0.045 \n",
            "[  31/ 195]                 blk.11.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  32/ 195]               blk.11.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  33/ 195]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  34/ 195]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  35/ 195]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  36/ 195]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  37/ 195]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 195]               blk.12.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.037 0.051 0.067 0.081 0.093 0.099 0.100 0.093 0.081 0.066 0.051 0.037 0.026 0.045 \n",
            "[  39/ 195]                 blk.12.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  40/ 195]               blk.12.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  41/ 195]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  42/ 195]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  43/ 195]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  44/ 195]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  45/ 195]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  46/ 195]                 blk.13.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  47/ 195]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  48/ 195]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  49/ 195]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  50/ 195]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  51/ 195]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  52/ 195]                blk.2.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  53/ 195]                  blk.2.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  54/ 195]                blk.2.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  55/ 195]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  56/ 195]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  57/ 195]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  58/ 195]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  59/ 195]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  60/ 195]                blk.3.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 \n",
            "[  61/ 195]                  blk.3.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  62/ 195]                blk.3.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  63/ 195]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  64/ 195]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  65/ 195]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  66/ 195]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  67/ 195]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 195]                blk.4.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[  69/ 195]                  blk.4.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  70/ 195]                blk.4.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  71/ 195]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  72/ 195]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  73/ 195]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  74/ 195]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  75/ 195]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 195]                blk.5.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  77/ 195]                  blk.5.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  78/ 195]                blk.5.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  79/ 195]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  80/ 195]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  81/ 195]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  82/ 195]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 195]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  84/ 195]                blk.6.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[  85/ 195]                  blk.6.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  86/ 195]                blk.6.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  87/ 195]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  88/ 195]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  89/ 195]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  90/ 195]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  91/ 195]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 195]                blk.7.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[  93/ 195]                  blk.7.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  94/ 195]                blk.7.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[  95/ 195]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[  96/ 195]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  97/ 195]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  98/ 195]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  99/ 195]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 100/ 195]                blk.8.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 101/ 195]                  blk.8.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 102/ 195]                blk.8.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 103/ 195]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 104/ 195]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 105/ 195]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 106/ 195]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 107/ 195]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 108/ 195]                blk.9.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.099 0.100 0.093 0.081 0.067 0.051 0.037 0.026 0.045 \n",
            "[ 109/ 195]                  blk.9.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 110/ 195]                blk.9.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 111/ 195]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, quantizing to q5_K .. size =   593.50 MiB ->   204.02 MiB\n",
            "[ 112/ 195]                        output.weight - [ 2048, 151936,     1,     1], type =    f16, quantizing to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[ 113/ 195]               blk.13.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.100 0.093 0.081 0.067 0.051 0.037 0.026 0.045 \n",
            "[ 114/ 195]               blk.13.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 115/ 195]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 116/ 195]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 117/ 195]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 118/ 195]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 119/ 195]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 120/ 195]               blk.14.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 121/ 195]                 blk.14.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 122/ 195]               blk.14.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 123/ 195]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 124/ 195]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 125/ 195]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 126/ 195]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 127/ 195]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 195]               blk.15.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[ 129/ 195]                 blk.15.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 130/ 195]               blk.15.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 131/ 195]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 132/ 195]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 133/ 195]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 134/ 195]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 135/ 195]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 136/ 195]               blk.16.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[ 137/ 195]                 blk.16.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 138/ 195]               blk.16.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 139/ 195]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 140/ 195]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 141/ 195]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 142/ 195]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 143/ 195]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 144/ 195]               blk.17.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.226 0.106 0.088 0.066 0.048 0.031 0.020 0.027 \n",
            "[ 145/ 195]                 blk.17.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 146/ 195]               blk.17.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 147/ 195]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 148/ 195]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 149/ 195]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 150/ 195]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 151/ 195]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 195]               blk.18.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[ 153/ 195]                 blk.18.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 154/ 195]               blk.18.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 155/ 195]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 156/ 195]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 157/ 195]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 158/ 195]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 159/ 195]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 195]               blk.19.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
            "quantizing to q5_1 .. size =    21.50 MiB ->     8.06 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 \n",
            "[ 161/ 195]                 blk.19.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 162/ 195]               blk.19.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 163/ 195]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 164/ 195]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 165/ 195]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 166/ 195]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 167/ 195]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 168/ 195]               blk.20.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 169/ 195]                 blk.20.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 170/ 195]               blk.20.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 171/ 195]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 172/ 195]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 173/ 195]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 174/ 195]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 175/ 195]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 176/ 195]               blk.21.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 177/ 195]                 blk.21.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 178/ 195]               blk.21.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 179/ 195]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 180/ 195]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 181/ 195]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 182/ 195]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 183/ 195]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 195]               blk.22.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.020 0.027 \n",
            "[ 185/ 195]                 blk.22.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 186/ 195]               blk.22.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 187/ 195]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB\n",
            "[ 188/ 195]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, quantizing to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 189/ 195]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 190/ 195]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 191/ 195]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 192/ 195]               blk.23.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
            "\n",
            "get_k_quant_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "quantizing to q8_0 .. size =    21.50 MiB ->    11.42 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.063 0.087 0.110 0.247 0.110 0.087 0.063 0.044 0.029 0.018 0.026 \n",
            "[ 193/ 195]                 blk.23.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 194/ 195]               blk.23.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, quantizing to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
            "[ 195/ 195]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  3503.95 MB\n",
            "llama_model_quantize_internal: quant size  =  1339.20 MB\n",
            "llama_model_quantize_internal: hist: 0.022 0.027 0.029 0.041 0.057 0.074 0.090 0.103 0.164 0.100 0.084 0.066 0.049 0.034 0.023 0.036 \n",
            "llama_model_quantize_internal: WARNING: 24 of 122 tensor(s) incompatible with k-quants and required fallback quantization\n",
            "\n",
            "main: quantize time = 73285.75 ms\n",
            "main:    total time = 73285.75 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yxQ6mi9vc1SW",
        "outputId": "d6cd7c02-f380-413b-c041-5dea5a2abaf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/qwen-1_8-fine-tune-model-loss/ggml-model-Q5_K_M.gguf   \"/content/drive/My Drive/\"\n"
      ],
      "metadata": {
        "id": "v7zs-CDKc_Ne"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
