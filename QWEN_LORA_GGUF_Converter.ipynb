{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/QWEN_LORA_GGUF_Converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "python3 --version\n",
        "make --version\n",
        "g++ --version"
      ],
      "metadata": {
        "id": "ysh3Cx9hSGY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bb003dHqcQdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f95154-0cdb-4595-9a2f-8609efd8054e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 16673, done.\u001b[K\n",
            "remote: Counting objects: 100% (5542/5542), done.\u001b[K\n",
            "remote: Compressing objects: 100% (368/368), done.\u001b[K\n",
            "remote: Total 16673 (delta 5333), reused 5298 (delta 5173), pack-reused 11131\u001b[K\n",
            "Receiving objects: 100% (16673/16673), 18.86 MiB | 20.63 MiB/s, done.\n",
            "Resolving deltas: 100% (11609/11609), done.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "\u001b[01m\u001b[Kcommon/common.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool gpt_params_parse_ex(int, char**, gpt_params&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kcommon/common.cpp:223:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  223 |             file.read(\u001b[01;35m\u001b[K(char *)params.prompt.data()\u001b[m\u001b[K, size);\n",
            "      |                       \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "\u001b[01m\u001b[Kexamples/perplexity/perplexity.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool deserialize_string(std::istream&, std::string&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/perplexity/perplexity.cpp:1038:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 1038 |         if (!in.read(\u001b[01;35m\u001b[K(char *)str.data()\u001b[m\u001b[K, size).fail()) return true;\n",
            "      |                      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/imatrix/imatrix.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o imatrix  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Requirement already satisfied: numpy~=1.24.4 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 1)) (1.24.4)\n",
            "Requirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.35.2)\n",
            "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 4)) (0.6.0)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 5)) (4.25.2)\n",
            "Requirement already satisfied: torch~=2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.20.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "%%shell\n",
        "! rm -rf  /content/llama.cpp/\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp && make && pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "# !git clone https://huggingface.co/Qwen/Qwen-1_8B\n",
        "!git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkodq6Z4OCOy",
        "outputId": "1a2778e6-e330-4450-d6fa-9e455701d107"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Qwen-1_8B-Chat'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 58 (delta 18), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (58/58), 2.58 MiB | 7.56 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 3.42 GiB | 60.47 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ENwsZ0_fYMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9569d10-86b2-44d0-cd20-023489296beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-transfer\n",
            "  Downloading hf_transfer-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-transfer\n",
            "Successfully installed hf-transfer-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install hf-transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66mMgKSfbbw",
        "outputId": "f51cf5e0-d48a-4f81-d235-34bd35f5a996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/.gitattributes to /root/.cache/huggingface/hub/tmpiyvj13ao\n",
            "\r.gitattributes:   0% 0.00/1.52k [00:00<?, ?B/s]\r.gitattributes: 100% 1.52k/1.52k [00:00<00:00, 3.73MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/README.md to /root/.cache/huggingface/hub/tmp3q00pxti\n",
            "\rREADME.md:   0% 0.00/8.73k [00:00<?, ?B/s]\rREADME.md: 100% 8.73k/8.73k [00:00<00:00, 24.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/Research%20License.docx to /root/.cache/huggingface/hub/tmppc3ttgr8\n",
            "Research License.docx: 100% 38.9k/38.9k [00:00<00:00, 29.6MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/added_tokens.json to /root/.cache/huggingface/hub/tmp04i_95c4\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 4.41MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/config.json to /root/.cache/huggingface/hub/tmpdixevjjl\n",
            "config.json: 100% 732/732 [00:00<00:00, 2.76MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/configuration_phi.py to /root/.cache/huggingface/hub/tmpalphsppd\n",
            "configuration_phi.py: 100% 2.03k/2.03k [00:00<00:00, 6.01MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/generation_config.json to /root/.cache/huggingface/hub/tmp2oajp464\n",
            "generation_config.json: 100% 69.0/69.0 [00:00<00:00, 245kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/merges.txt to /root/.cache/huggingface/hub/tmpodezm9of\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 20.2MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/modeling_phi.py to /root/.cache/huggingface/hub/tmp1aus7yci\n",
            "modeling_phi.py: 100% 33.5k/33.5k [00:00<00:00, 63.7MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/pytorch_model.bin to /root/.cache/huggingface/hub/tmpimgts1rq\n",
            "pytorch_model.bin: 100% 2.84G/2.84G [00:20<00:00, 136MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/special_tokens_map.json to /root/.cache/huggingface/hub/tmp_cic1_yz\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 405kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer.json to /root/.cache/huggingface/hub/tmpr1dlp5fq\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 27.8MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer_config.json to /root/.cache/huggingface/hub/tmpvazjcj8u\n",
            "tokenizer_config.json: 100% 237/237 [00:00<00:00, 993kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/vocab.json to /root/.cache/huggingface/hub/tmpi0gu_jjc\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 50.4MB/s]\n",
            "/content/phi-1_5\n"
          ]
        }
      ],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli download microsoft/phi-1_5 --local-dir phi-1_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drRiT7wPhgA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738395c1-c037-4cd6-e507-e193e9754661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: phi-1_5\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 50000 merge(s).\n",
            "gguf: Setting special token type bos to 50256\n",
            "gguf: Setting special token type eos to 50256\n",
            "gguf: Setting special token type unk to 50256\n",
            "Exporting model to 'phi-1_5/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'pytorch_model.bin'\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "token_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "output.weight, n_dims = 2, torch.float16 --> float16\n",
            "output.bias, n_dims = 1, torch.float16 --> float32\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 1173, in <module>\n",
            "    model_instance.write()\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 139, in write\n",
            "    self.gguf_writer.write_tensors_to_file()\n",
            "  File \"/content/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 264, in write_tensors_to_file\n",
            "    tensor.tofile(self.fout)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 llama.cpp/convert-hf-to-gguf.py ./phi-1_5/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHwHI-d1Ojv1",
        "outputId": "a7c5effd-accd-42aa-fe9e-f222d909a5ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 llama.cpp/convert-hf-to-gguf.py ./Qwen-1_8B-Chat/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCL8hb-XOVCu",
        "outputId": "0fe390ed-7cbd-4721-e30e-714d8c53ee47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen-1_8B-Chat\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 151387 merge(s).\n",
            "gguf: Setting special token type bos to 151643\n",
            "gguf: Setting special token type eos to 151643\n",
            "gguf: Setting special token type unk to 151643\n",
            "Exporting model to 'Qwen-1_8B-Chat/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.1.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.10.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.11.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.12.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.13.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.2.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.3.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.4.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.5.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.6.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.7.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.8.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.9.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "token_embd.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.14.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.15.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.16.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.17.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.18.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.19.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.20.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.21.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.22.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "blk.23.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
            "output_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
            "Model successfully exported to 'Qwen-1_8B-Chat/ggml-model-f16.gguf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd llama.cpp\n",
        "git log | head -1\n",
        "python3 --version\n",
        "pip list | egrep \"torch|numpy|sentencepiece\"\n",
        "make --version | head -1\n",
        "\n",
        "md5sum ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
        "md5sum ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvv4fmVkUEC0",
        "outputId": "94c6b865-df9d-4ca6-a253-706359a8f891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commit 708e179e8562c2604240df95a2241dea17fd808b\n",
            "Python 3.10.12\n",
            "numpy                            1.24.4\n",
            "sentencepiece                    0.1.98\n",
            "torch                            2.1.0+cu121\n",
            "torchaudio                       2.1.0+cu121\n",
            "torchdata                        0.7.0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.16.0\n",
            "torchvision                      0.16.0+cu121\n",
            "GNU Make 4.3\n",
            "83af84a363c8593a8ed930b4033c4868  ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
            "706ead5763dbc23b9a1e1388ce7721ee  ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMeEnRZpcfoa"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q8_0\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q6_K\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q5_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q4_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q3_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q2_K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd dolphin-2_6-phi-2 && rename 's/^ggml-model-/dolphin-2_6-phi-2-/' ggml-model-*"
      ],
      "metadata": {
        "id": "vsKWnTLw5FV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli upload dolphin-2_6-phi-2-GGUF ./dolphin-2_6-phi-2/ ."
      ],
      "metadata": {
        "id": "JEIYZV9Z5OC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX-sZuxxrkYV"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/main -m ./dolphin-2_6-phi-2/ggml-model-Q4_K_M.gguf -p \"What's up?\" -n 400 -e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !llama.cpp/main -m Qwen-1_8B-Chat/ggml-model-f16.gguf -n 512 -p \"你叫什么\"  --chatml\n",
        "# !llama.cpp/main -m Qwen-1_8B-Chat/ggml-model-f16.gguf --lora ggml-adapter-model.bin -n 512 -p \"你是一个人工智能助手\" --chatml\n",
        "!llama.cpp/main -m Qwen-1_8B-Chat/ggml-model-f16.gguf -n 512 -p \"你是一个人工智能助手\" --chatml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JYiIzMEO8th",
        "outputId": "aabc355a-e763-446d-cb87-47450e4821d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 1950 (b2d80e10)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1705929266\n",
            "llama_model_loader: loaded meta data with 17 key-value pairs and 195 tensors from Qwen-1_8B-Chat/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
            "llama_model_loader: - kv   2:                        qwen.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                           qwen.block_count u32              = 24\n",
            "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
            "llama_model_loader: - type  f32:   73 tensors\n",
            "llama_model_loader: - type  f16:  122 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_layer          = 24\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
            "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = F16 (guessed)\n",
            "llm_load_print_meta: model params     = 1.84 B\n",
            "llm_load_print_meta: model size       = 3.42 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = Qwen\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_tensors: ggml ctx size =    0.07 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/25 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  3503.95 MiB\n",
            "....................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "llama_new_context_with_model:        CPU compute buffer size =   300.75 MiB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "Reverse prompt: '<|im_start|>user\n",
            "'\n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 8\n",
            "\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to LLaMa.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            "\n",
            "system\n",
            "你是一个人工智能助手\n",
            "> \n",
            "\n",
            "llama_print_timings:        load time =     639.52 ms\n",
            "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings: prompt eval time =    1092.31 ms /     8 tokens (  136.54 ms per token,     7.32 tokens per second)\n",
            "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:       total time =    2714.34 ms /     9 tokens\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}