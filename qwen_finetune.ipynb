{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+xMDlnSzULYP+gt+Mf8Kd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/qwen_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdJeKM2_c3lF"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-50AbTRLCh5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13599929-29c2-4d88-de57-493ee357c895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'qwen-finetune-demo'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 91 (delta 25), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (91/91), 4.21 MiB | 487.00 KiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"下载代码\n",
        "\"\"\"\n",
        "%cd /content/\n",
        "! rm -rf  /content/qwen-finetune-demo/\n",
        "!git clone https://gitee.com/duanshuyong/qwen-finetune-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/\n",
        "!ls\n",
        "!pip install -U -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0684LhvlfEbz",
        "outputId": "a9aa74f7-f1a4-4704-e0a5-f0f73aa74849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev\n",
            "args.MD  config  data_processer.py  data_utils.py  LICENSE    requirements.txt\ttraining\n",
            "assets\t data\t data_tools.py\t    infer\t   README.MD  scripts\t\ttrain.py\n",
            "Collecting aigc_zoo (from -r requirements.txt (line 12))\n",
            "  Cloning https://github.com/ssbuild/aigc_zoo to /tmp/pip-install-p9tr__cp/aigc-zoo_112b1a599db14e8da53108966578ff51\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ssbuild/aigc_zoo /tmp/pip-install-p9tr__cp/aigc-zoo_112b1a599db14e8da53108966578ff51\n",
            "  Resolved https://github.com/ssbuild/aigc_zoo to commit b660336b7dde41df7582a3d360e24bd5f8e8b240\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cohere (from -r requirements.txt (line 1))\n",
            "  Downloading cohere-4.44-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai (from -r requirements.txt (line 2))\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from -r requirements.txt (line 3))\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 4))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.35.2)\n",
            "Collecting transformers>=4.30 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed (from -r requirements.txt (line 6))\n",
            "  Downloading deepspeed-0.12.6.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cpm-kernels (from -r requirements.txt (line 7))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes>=0.39 (from -r requirements.txt (line 8))\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.20 (from -r requirements.txt (line 9))\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere->-r requirements.txt (line 1)) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere->-r requirements.txt (line 1))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere->-r requirements.txt (line 1))\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere->-r requirements.txt (line 1))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 2)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (4.66.1)\n",
            "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openai (from -r requirements.txt (line 2))\n",
            "  Downloading openai-1.7.2-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openai-1.7.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openai-1.7.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openai-1.6.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r requirements.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30->-r requirements.txt (line 5)) (0.4.1)\n",
            "Collecting hjson (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 6)) (9.0.0)\n",
            "Collecting pynvml (from deepspeed->-r requirements.txt (line 6))\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 6)) (2.1.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.39->-r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting deep_training~=0.2.10 (from aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading deep_training-0.2.10.post0-py3-none-any.whl (863 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.6/863.6 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.0)\n",
            "Collecting lightning<50.0,>=2.0 (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy-io<0.1.0,>=0.0.10 (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading numpy_io-0.0.10-py3-none-any.whl (34 kB)\n",
            "Collecting sentencepiece (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqmetric (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading seqmetric-0.1.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (2.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.16.0)\n",
            "Collecting fastdatasets>=0.9.17 (from deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading fastdatasets-0.9.17-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.30->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere->-r requirements.txt (line 1)) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed->-r requirements.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed->-r requirements.txt (line 6)) (2.1.0)\n",
            "Collecting tfrecords<0.3,>=0.2.16 (from fastdatasets>=0.9.17->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading tfrecords-0.2.17-cp310-cp310-manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting data-serialize>=0.2.1 (from fastdatasets>=0.9.17->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading data_serialize-0.2.2-py3-none-any.whl (12 kB)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<50.0,>=2.0->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<50.0,>=2.0->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning<50.0,>=2.0->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12))\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->deep_training~=0.2.10->aigc_zoo->-r requirements.txt (line 12)) (3.2.2)\n",
            "Building wheels for collected packages: deepspeed, aigc_zoo\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.12.6-py3-none-any.whl size=1306735 sha256=d087c0d4e426e6619a544c4e979e6a7e560b7bc132743dea27ed9c894002eb53\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/dc/a2/f585faaed4dec84108916dcc8e8a7c129a216df8202ca32984\n",
            "  Building wheel for aigc_zoo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aigc_zoo: filename=aigc_zoo-0.2.10-py3-none-any.whl size=209310 sha256=ef094eda937f680f73887805c188d67d4161b05ec3ec0b4d37dee0c5ada3999c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-grfb7pln/wheels/cb/eb/d2/284d314787ce93e7718036dc11d8be5b507e9277fa5e30ac56\n",
            "Successfully built deepspeed aigc_zoo\n",
            "Installing collected packages: sentencepiece, ninja, hjson, cpm-kernels, tfrecords, seqmetric, pynvml, lightning-utilities, importlib_metadata, h11, fastavro, einops, data-serialize, backoff, tiktoken, httpcore, fastdatasets, bitsandbytes, torchmetrics, numpy-io, httpx, deepspeed, cohere, accelerate, transformers, pytorch-lightning, openai, lightning, deep_training, aigc_zoo\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.26.1 aigc_zoo-0.2.10 backoff-2.2.1 bitsandbytes-0.42.0 cohere-4.44 cpm-kernels-1.0.11 data-serialize-0.2.2 deep_training-0.2.10.post0 deepspeed-0.12.6 einops-0.7.0 fastavro-1.9.3 fastdatasets-0.9.17 h11-0.14.0 hjson-3.1.0 httpcore-1.0.2 httpx-0.26.0 importlib_metadata-6.11.0 lightning-2.1.3 lightning-utilities-0.10.1 ninja-1.11.1.1 numpy-io-0.0.10 openai-1.5.0 pynvml-11.5.0 pytorch-lightning-2.1.3 sentencepiece-0.1.99 seqmetric-0.1.2 tfrecords-0.2.17 tiktoken-0.5.2 torchmetrics-1.3.0.post0 transformers-4.36.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %cd scripts\n",
        " !bash train_full.sh -m dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbTIH3EoQX2G",
        "outputId": "6c7c31c4-ca01-4a44-c100-7cf4de95e114"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/scripts\n",
            "2024-01-19 06:01:08.673449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-19 06:01:08.673518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-19 06:01:08.674825: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-19 06:01:09.916280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 389, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/nlp/pre_models/torch/qwen/Qwen-7B-Chat'. Use `repo_type` argument if needed.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/qwen-finetune-demo/qwen_finetuning-dev/scripts/../data_utils.py\", line 278, in <module>\n",
            "    tokenizer, config, _,_ = dataHelper.load_tokenizer_and_config(tokenizer_class_name=QWenTokenizer,config_class_name=QWenConfig)\n",
            "  File \"/content/qwen-finetune-demo/qwen_finetuning-dev/scripts/../data_utils.py\", line 89, in load_tokenizer_and_config\n",
            "    return super().load_tokenizer_and_config(*args, tokenizer_kwargs=tokenizer_kwargs, config_kwargs=config_kwargs,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deep_training/data_helper/data_helper.py\", line 190, in load_tokenizer_and_config\n",
            "    tokenizer = load_tokenizer(tokenizer_name=tokenizer_name or model_args.tokenizer_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy_io/pytorch_loader/tokenizer_config_helper.py\", line 37, in load_tokenizer\n",
            "    tokenizer = class_name.from_pretrained(tokenizer_name or model_name_or_path, **tokenizer_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 1951, in from_pretrained\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 454, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: Incorrect path_or_model_id: '/data/nlp/pre_models/torch/qwen/Qwen-7B-Chat'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hvc7g5iMRZ82"
      }
    }
  ]
}