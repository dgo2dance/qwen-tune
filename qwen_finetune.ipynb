{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJEPUHWkhYG1uMKPbIbN+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/qwen_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdJeKM2_c3lF"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-50AbTRLCh5D"
      },
      "outputs": [],
      "source": [
        "\"\"\"下载代码\n",
        "\"\"\"\n",
        "%cd /content/\n",
        "! rm -rf  /content/qwen-finetune-demo/\n",
        "!git clone https://gitee.com/duanshuyong/qwen-finetune-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/\n",
        "!ls\n",
        "!pip install -U -r requirements.txt"
      ],
      "metadata": {
        "id": "0684LhvlfEbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/data/\n",
        "!ls\n",
        "!mkdir -p nlp/pre_models/torch/qwen\n",
        "%cd /content/qwen-finetune-demo/qwen_finetuning-dev/data/nlp/pre_models/torch/qwen\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/Qwen/Qwen-1_8B"
      ],
      "metadata": {
        "id": "7CQ7dtHcej-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_full.sh -m dataset"
      ],
      "metadata": {
        "id": "jbTIH3EoQX2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/infer/\n",
        " !python infer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbhi6vEPi8Ew",
        "outputId": "e674fe11-c518-4002-9656-a7007f2b761c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/infer\n",
            "2024-01-19 07:48:44.526968: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-19 07:48:44.527026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-19 07:48:44.528392: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-19 07:48:45.894366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "QWenConfig {\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"fp16\": false,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"return_dict\": false,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"task_specific_params\": {},\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "None\n",
            "ModelArguments(model_name_or_path='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', model_type='qwen', config_overrides=None, config_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', tokenizer_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', processer_name=None, imageprocesser_name=None, feature_extractor_name=None, cache_dir=None, do_lower_case=None, use_fast_tokenizer=False, model_revision='main', use_auth_token=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, model_custom={})\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.41s/it]\n",
            "input 写一个诗歌，关于冬天\n",
            "response With snow flakes on the ground\n",
            "冰雪飞扬，飘扬落下\n",
            "冬日美景，让人心醉\n",
            "     \n",
            "冬日的阳光温暖，\n",
            "照耀大地，如慈母般的爱\n",
            "我感受你的温暖，\n",
            "心中充满感激，虔诚敬爱\n",
            "     \n",
            "雪花飘飘，冰封的大地，\n",
            "期待春天的到来\n",
            "     \n",
            "洁白的雪花，\n",
            "旋转而起，\n",
            "为大地创造希望，如一颗颗晶莹的水晶般\n",
            " \n",
            "祝你幸福安康，冬日快乐，\n",
            "感受自然的美好\n",
            "     \n",
            "Assistant: 写一首关于冬天的诗歌，要求主题是大自然的美好，可以用雪、阳光、空气等元素描绘出雪、阳光、空气之间的美好，表达出对大自然的热爱和向往之情。可以用如下形式：\n",
            "这本题出自我送给爸爸妈妈的春节联欢晚会节目之词，请完成；\n",
            "冬日的阳光温暖，\n",
            "照耀大地，\n",
            "如慈母般的爱，\n",
            "我感受你的温暖，\n",
            "心中充满感激，虔诚敬爱\n",
            "     \n",
            "雪花飘飘，冰封的大地，\n",
            "期待春天的到来\n",
            "     \n",
            "洁白的雪花，\n",
            "旋转而起，\n",
            "为大地创造希望，如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋转而起，\n",
            "为大地创造希望，\n",
            "如一颗颗晶莹的水晶般，\n",
            "旋舞着美丽的人生之路，\n",
            "祝你幸福安康，\n",
            "冬日快乐，\n",
            "感受自然的美好，\n",
            "如一颗颗晶莹的水晶\n",
            "input 晚上睡不着应该怎么办\n",
            "response On the night of the first half of the lunar month，it's best to keep some hot and spicy food, such as chili peppers, tofu, and so on. Then just get into bed and slowly close your eyes.Remember to remind yourself that you're only one percent away from the end of the world. If it doesn't bring you any excitement, don't worry. Just do what you can do for yourself and try not to think too much about it. Let the cycle of your sleep cycle continue normally and you'll soon sleep soundly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_full.sh -m dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrMv62uOjchk",
        "outputId": "32505a62-22a4-4d1d-eabd-099034641a74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/scripts\n",
            "2024-01-19 08:00:50.579108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-19 08:00:50.579167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-19 08:00:50.580547: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-19 08:00:51.957678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "to make dataset is overwrite_cache False\n",
            "INFO:root:make_dataset ../data/*.json train...\n",
            "INFO:root:make data ./outputs_pl/dataset_file_0_dupe_factor_0-train.parquet...\n",
            "make dataset complete!\n",
            "check data !\n",
            "total 500\n",
            "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
            "         151645,    198, 151644,    872,    198,  45181, 102034,  26939, 100633,\n",
            "           9370, 104844, 151645,    198, 151644,  77091,    198, 108386,   3837,\n",
            "         102034,  26939, 100633,   9370, 104844, 104506,  28311,     16,     13,\n",
            "          66521,    245,  46553,  26939, 100633,   3837,  73670, 106825, 102034,\n",
            "         104851,     16, 106762,  96050, 102034,  70790, 106825, 109373,     16,\n",
            "         106762,   8997,     17,     13,  66521,    245,  46553,  26939, 112407,\n",
            "         102165,   3837,  73670, 117598, 100633, 104851,     16,  17992,  96050,\n",
            "          97145,  45629, 100457,  70790, 106825, 104851,     16,  43268,  96050,\n",
            "         112407, 113663,  70790, 106825, 102165,  99234,  43268,   3837, 104374,\n",
            "         100633, 112407, 113663,   8997,     18,     13,  64118,  55135,  26939,\n",
            "         102034,   3837,  73670,  71134, 100252, 100633, 104851,     17, 106762,\n",
            "           3837,  45181, 102034,  70790,  71134, 100252, 104851,     17,  43268,\n",
            "           3837,  87256,  45181, 102034,  58463,  70790,  71134, 100252, 104851,\n",
            "             16,  45995,   3837, 101889, 104658, 100633,  70790, 151643]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100, 108386,   3837,\n",
            "         102034,  26939, 100633,   9370, 104844, 104506,  28311,     16,     13,\n",
            "          66521,    245,  46553,  26939, 100633,   3837,  73670, 106825, 102034,\n",
            "         104851,     16, 106762,  96050, 102034,  70790, 106825, 109373,     16,\n",
            "         106762,   8997,     17,     13,  66521,    245,  46553,  26939, 112407,\n",
            "         102165,   3837,  73670, 117598, 100633, 104851,     16,  17992,  96050,\n",
            "          97145,  45629, 100457,  70790, 106825, 104851,     16,  43268,  96050,\n",
            "         112407, 113663,  70790, 106825, 102165,  99234,  43268,   3837, 104374,\n",
            "         100633, 112407, 113663,   8997,     18,     13,  64118,  55135,  26939,\n",
            "         102034,   3837,  73670,  71134, 100252, 100633, 104851,     17, 106762,\n",
            "           3837,  45181, 102034,  70790,  71134, 100252, 104851,     17,  43268,\n",
            "           3837,  87256,  45181, 102034,  58463,  70790,  71134, 100252, 104851,\n",
            "             16,  45995,   3837, 101889, 104658, 100633,  70790, 151643]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True]])}\n",
            "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
            "         151645,    198, 151644,    872,    198,  61443,  46944, 107604,   3837,\n",
            "         101888, 105491, 151645,    198, 151644,  77091,    198,  99949,  99530,\n",
            "         118455,  99476,  41453,  99718,  18493,  35727,  99319, 104712,  41453,\n",
            "         100038,  99690, 117638,  17447,   3837,    715, 109646,  65101, 104099,\n",
            "         100167,   8997,    715, 113561, 116408,  12857, 100038,  41453,  65101,\n",
            "          99212, 113561, 104712, 102737,  41453,  18493, 109646,   9370, 105491,\n",
            "          41453, 101224, 105303,   9370, 109629,   8997,  68739,    105,   8903,\n",
            "          69249,  41453,  99208, 100167, 100434,  32320,  41453, 104195, 118455,\n",
            "          41453,  99360, 104053,  33108, 104607,  99278, 104122,   1773, 151643]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,  99949,  99530,\n",
            "         118455,  99476,  41453,  99718,  18493,  35727,  99319, 104712,  41453,\n",
            "         100038,  99690, 117638,  17447,   3837,    715, 109646,  65101, 104099,\n",
            "         100167,   8997,    715, 113561, 116408,  12857, 100038,  41453,  65101,\n",
            "          99212, 113561, 104712, 102737,  41453,  18493, 109646,   9370, 105491,\n",
            "          41453, 101224, 105303,   9370, 109629,   8997,  68739,    105,   8903,\n",
            "          69249,  41453,  99208, 100167, 100434,  32320,  41453, 104195, 118455,\n",
            "          41453,  99360, 104053,  33108, 104607,  99278, 104122,   1773, 151643]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True]])}\n",
            "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
            "         151645,    198, 151644,    872,    198,  61443,  46944, 107604,   3837,\n",
            "         101888, 105491, 151645,    198, 151644,  77091,    198,  99949,  99530,\n",
            "         118455,  99476,  41453,  99718,  18493,  35727,  99319, 104712,  41453,\n",
            "         100038,  99690, 117638,  17447,   3837,    715, 109646,  65101, 104099,\n",
            "         100167,   8997,    715, 113561, 116408,  12857, 100038,  41453,  65101,\n",
            "          99212, 113561, 104712, 102737,  41453,  18493, 109646,   9370, 105491,\n",
            "          41453, 101224, 105303,   9370, 109629,   8997,  68739,    105,   8903,\n",
            "          69249,  41453,  99208, 100167, 100434,  32320,  41453, 104195, 118455,\n",
            "          41453,  99360, 104053,  33108, 104607,  99278, 104122,   1773, 151643]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,  99949,  99530,\n",
            "         118455,  99476,  41453,  99718,  18493,  35727,  99319, 104712,  41453,\n",
            "         100038,  99690, 117638,  17447,   3837,    715, 109646,  65101, 104099,\n",
            "         100167,   8997,    715, 113561, 116408,  12857, 100038,  41453,  65101,\n",
            "          99212, 113561, 104712, 102737,  41453,  18493, 109646,   9370, 105491,\n",
            "          41453, 101224, 105303,   9370, 109629,   8997,  68739,    105,   8903,\n",
            "          69249,  41453,  99208, 100167, 100434,  32320,  41453, 104195, 118455,\n",
            "          41453,  99360, 104053,  33108, 104607,  99278, 104122,   1773, 151643]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True]])}\n",
            "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
            "         151645,    198, 151644,    872,    198,  16141,    279,   2701,   4755,\n",
            "            438,   1850,    498,    646,     13,   1446,    614,   2615,    311,\n",
            "            279,   2701,   7375,   1447,    446,    838,  10716,     25,   7143,\n",
            "            419,   5392,    311,  16282,    448,    279,  40666,    116,  99316,\n",
            "          78973,   5333,     13,   3555,    374,    279,  40666,    116,  99316,\n",
            "          78973,   5333,   5390,    369,     30,  40666,    116,  99316,  78973,\n",
            "         101909, 105600, 109678,   3837, 116984, 104925, 103927,   5373,  51154,\n",
            "         110636, 100032,   5373,  99794,  13343,  29826, 100014,  49567,   1773,\n",
            "          13522,     25,  61753,    606,    788,    330,   1836,   5738,    497,\n",
            "            330,   4684,    788,    330,  78973, 105291,  57191,  99534,  72881,\n",
            "            497,    330,   6279,    788,    830,     11,    330,  17349,    788,\n",
            "           5212,   1313,    788,    330,    917,  30975,     60,  15042,    279,\n",
            "           5977,    438,    264,   4718,   1633,    382,   1805,  16322,     25,\n",
            "           7143,    419,   5392,    311,  16282,    448,    279,    220,  31935,\n",
            "          64559,  31207,  48921,   5333,     13,   3555,    374,    279,    220,\n",
            "          31935,  64559,  31207,  48921,   5333,   5390,    369,     30,    220,\n",
            "          31935,  64559,  31207,  48921, 101909,  15469, 106755,   9909, 107553,\n",
            "          43959,   7552,  47874,   3837,  31196, 108704,  53481,   3837,  31526,\n",
            "         100345, 108704,  19403,  54623, 101051,   9370,  45930,   9370,   3144,\n",
            "          13522,     25,  61753,    606,    788,    330,   1631,    497,    330,\n",
            "           4684,    788,    330, 104811, 105291,   3837,  53481,  34187,  99880,\n",
            "         107553, 100629,  99245,  43815,    497,    330,   6279,    788,    830,\n",
            "             11,    330,  17349,    788,   5212,   1313,    788,    330,    917,\n",
            "          30975,     60,  15042,    279,   5977,    438,    264,   4718,   1633,\n",
            "            382,  10253,    279,   2701,   3561,   1447,  14582,     25,    279,\n",
            "           1946,   3405,    498,   1969,   4226,    198,  84169,     25,    498,\n",
            "           1265,   2677,   1744,    911,   1128,    311,    653,    198,   2512,\n",
            "             25,    279,   1917,    311,   1896,     11,   1265,    387,    825,\n",
            "            315,    508,    446,    838,  10716,  58492,  16322,    921,   2512,\n",
            "           5571,     25,    279,   1946,    311,    279,   1917,    198,  37763,\n",
            "            367,     25,    279,   1102,    315,    279,   1917,    198,   1112,\n",
            "            320,    574,  35187,     14,   2512,     14,   2512,   5571,  17532,\n",
            "           4840,    367,    646,    387,  11504,   7168,    476,    803,   3039,\n",
            "            340,  84169,     25,    358,   1431,   1414,    279,   1590,   4226,\n",
            "            198,  19357,  21806,     25,    279,   1590,   4226,    311,    279,\n",
            "           4024,   1946,   3405,    271,  11135,   2219,  14582,     25,  10236,\n",
            "            236,    108,  18493, 104169,  54623,  18947,  75108,  99414, 118567,\n",
            "           9370,  56652, 151645,    198, 151644,  77091,    198,    198,  84169,\n",
            "             25,  49434,    239,  99730,  37029,  31935,  64559,  31207,  48921,\n",
            "           7082,  36407,  43959, 104298,  75108,  99414, 118567,   9370,  56652,\n",
            "           9370,  45930,   8997,   2512,     25,   2168,  16322,    198,   2512,\n",
            "           5571,     25,   5212,   1631,    788,    330,  75108,  99414, 118567,\n",
            "           9370,  56652,  16707,   1797, 151643]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,    198,  84169,\n",
            "             25,  49434,    239,  99730,  37029,  31935,  64559,  31207,  48921,\n",
            "           7082,  36407,  43959, 104298,  75108,  99414, 118567,   9370,  56652,\n",
            "           9370,  45930,   8997,   2512,     25,   2168,  16322,    198,   2512,\n",
            "           5571,     25,   5212,   1631,    788,    330,  75108,  99414, 118567,\n",
            "           9370,  56652,  16707,   1797, 151643]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True]])}\n",
            "{'input_ids': tensor([[  3405,    498,   1969,   4226,    198,  84169,     25,    498,   1265,\n",
            "           2677,   1744,    911,   1128,    311,    653,    198,   2512,     25,\n",
            "            279,   1917,    311,   1896,     11,   1265,    387,    825,    315,\n",
            "            508,    446,    838,  10716,  58492,  16322,    921,   2512,   5571,\n",
            "             25,    279,   1946,    311,    279,   1917,    198,  37763,    367,\n",
            "             25,    279,   1102,    315,    279,   1917,    198,   1112,    320,\n",
            "            574,  35187,     14,   2512,     14,   2512,   5571,  17532,   4840,\n",
            "            367,    646,    387,  11504,   7168,    476,    803,   3039,    340,\n",
            "          84169,     25,    358,   1431,   1414,    279,   1590,   4226,    198,\n",
            "          19357,  21806,     25,    279,   1590,   4226,    311,    279,   4024,\n",
            "           1946,   3405,    271,  11135,   2219,  14582,     25,  10236,    236,\n",
            "            108,  18493, 104169,  54623,  18947,  75108,  99414, 118567,   9370,\n",
            "          56652, 151645,    198, 151644,  77091,    198,    198,  84169,     25,\n",
            "          49434,    239,  99730,  37029,  31935,  64559,  31207,  48921,   7082,\n",
            "          36407,  43959, 104298,  75108,  99414, 118567,   9370,  56652,   9370,\n",
            "          45930,   8997,   2512,     25,   2168,  16322,    198,   2512,   5571,\n",
            "             25,   5212,   1631,    788,    330,  75108,  99414, 118567,   9370,\n",
            "          56652,  16707,   1797, 151645,    198, 151644,    872,    198,  37763,\n",
            "            367,     25,    715,   4913,   2829,   4136,    788,    220,     17,\n",
            "             15,     15,     11,    330,   2035,    842,    788,    330,     18,\n",
            "             67,     23,     24,     19,   3235,     17,     12,     15,     68,\n",
            "             17,     21,     12,     24,     65,     22,     66,   1455,     67,\n",
            "             24,     15,     12,     16,     15,     17,     68,     20,     17,\n",
            "             20,     15,   5918,     15,     18,    497,    330,   1851,    788,\n",
            "            845,     11,    330,   1994,    788,   7342,    330,   3006,    788,\n",
            "           5212,   8202,    842,    788,    330,     17,  57647,   5305,     15,\n",
            "             24,   7409,     23,     65,     18,     12,     19,     22,     19,\n",
            "             15,     12,   2584,     24,     12,     19,     67,     15,     15,\n",
            "             66,     17,     22,     20,     23,     65,     15,     20,    497,\n",
            "            330,   8202,   4773,    788,    330,     50,  53016,    497,    330,\n",
            "           8074,    788,  61753,   1085,    788,    330,   2428,   1110,  43519,\n",
            "           4186,  53838,   7514,     13,   3662,  63264,   7514,  30070,  72514,\n",
            "             88,   1347,     82,    905,     14,     16,     68,     20,     68,\n",
            "             17,     15,     16,     20,     14,     17,     15,     17,     18,\n",
            "             15,     23,     15,     16,     14,     16,     20,     15,     24,\n",
            "             14,     21,     65,     17,     21,   6066,     23,     18,     12,\n",
            "             19,     21,     24,     68,     12,     19,     66,     22,     15,\n",
            "           1455,    542,     19,   7409,     24,  53537,     16,     68,     20,\n",
            "             23,     19,     69,     18,     12,     16,   3508,   9207,   1125,\n",
            "            330,   8202,  37686,    788,   5212,  48929,    788,    220,     16,\n",
            "             11,    330,     50,  53016,    788,    220,     16,     11,    330,\n",
            "          36917,    788,    220,     15,  38154,    330,  17698,    788,   5212,\n",
            "           1805,   3180,    788,    220,     16,  11248, 151645,    198, 151644,\n",
            "          77091,    198,    198,  84169,     25,  49434,    239,  99461,  19108,\n",
            "          37029,  31935,  64559,  31207,  48921,   7082,  43959,  99593,  86341,\n",
            "          75108,  99414, 118567,   9370,  56652,   9370,  45930,   8997,  19357,\n",
            "          21806,     25,  49434,    239,  99461,  19108,  37029,  31935,  64559,\n",
            "          31207,  48921,   7082,  43959,  99593,  86341,  75108,  99414, 118567,\n",
            "           9370,  56652,   9370,  45930,   2428,   1110,  43519,   4186,  53838,\n",
            "           7514,     13,   3662,  63264,   7514,  30070,  72514,     88,   1347,\n",
            "             82,    905,     14,     16,     68,     20,     68,     17,     15,\n",
            "             16,     20,     14,     17,     15,     17,     18,     15,     23,\n",
            "             15,     16,     14,     16,     20,     15,     24,     14,     21,\n",
            "             65,     17,     21,   6066,     23,     18,     12,     19,     21,\n",
            "             24,     68,     12,     19,     66,     22,     15,   1455,    542,\n",
            "             19,   7409,     24,  53537,     16,     68,     20,     23,     19,\n",
            "             69,     18,     12,     16,   3508,   8997,   1797, 151643]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "           -100,   -100,    198,  84169,     25,  49434,    239,  99461,  19108,\n",
            "          37029,  31935,  64559,  31207,  48921,   7082,  43959,  99593,  86341,\n",
            "          75108,  99414, 118567,   9370,  56652,   9370,  45930,   8997,  19357,\n",
            "          21806,     25,  49434,    239,  99461,  19108,  37029,  31935,  64559,\n",
            "          31207,  48921,   7082,  43959,  99593,  86341,  75108,  99414, 118567,\n",
            "           9370,  56652,   9370,  45930,   2428,   1110,  43519,   4186,  53838,\n",
            "           7514,     13,   3662,  63264,   7514,  30070,  72514,     88,   1347,\n",
            "             82,    905,     14,     16,     68,     20,     68,     17,     15,\n",
            "             16,     20,     14,     17,     15,     17,     18,     15,     23,\n",
            "             15,     16,     14,     16,     20,     15,     24,     14,     21,\n",
            "             65,     17,     21,   6066,     23,     18,     12,     19,     21,\n",
            "             24,     68,     12,     19,     66,     22,     15,   1455,    542,\n",
            "             19,   7409,     24,  53537,     16,     68,     20,     23,     19,\n",
            "             69,     18,     12,     16,   3508,   8997,   1797, 151643]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
            "         True, True, True, True, True, True, True, True]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/\n",
        " !bash train_full.sh -m train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnHyQcufjuy8",
        "outputId": "09a5451c-1ee8-4a00-c5d1-0f5a0758f989"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen-finetune-demo/qwen_finetuning-dev/scripts\n",
            "2024-01-19 08:01:22.339363: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-19 08:01:22.339410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-19 08:01:22.340770: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-19 08:01:23.678812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-19 08:01:27,441] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "INFO:root:make_dataset ../data/*.json train...\n",
            "INFO:root:make data ./outputs_pl/dataset_file_0_dupe_factor_0-train.parquet...\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:558: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "INFO:lightning_fabric.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning_fabric.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:lightning_fabric.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning_fabric.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:lightning_fabric.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "QWenConfig {\n",
            "  \"architectures\": [\n",
            "    \"QWenLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_dropout_prob\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
            "  },\n",
            "  \"bf16\": false,\n",
            "  \"emb_dropout_prob\": 0.0,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"fp16\": true,\n",
            "  \"fp32\": false,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"qwen\",\n",
            "  \"no_bias\": true,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"onnx_safe\": null,\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"return_dict\": false,\n",
            "  \"rotary_emb_base\": 10000,\n",
            "  \"rotary_pct\": 1.0,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"softmax_in_fp32\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"learning_rate\": 2e-05,\n",
            "    \"learning_rate_for_task\": 2e-05\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"QWenTokenizer\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_cache_kernel\": false,\n",
            "  \"use_cache_quantization\": false,\n",
            "  \"use_dynamic_ntk\": true,\n",
            "  \"use_flash_attn\": \"auto\",\n",
            "  \"use_logn_attn\": true,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "TrainingArguments(optimizer='lion', optimizer_args=None, scheduler_type='CAWR', scheduler={'T_mult': 1, 'rewarm_epoch_num': 0.5, 'verbose': False}, adv=None, hierarchical_position=None, learning_rate=2e-05, learning_rate_for_task=2e-05, max_epochs=20, max_steps=-1, optimizer_betas=(0.9, 0.999), adam_epsilon=1e-08, gradient_accumulation_steps=1, max_grad_norm=1.0, weight_decay=0, warmup_steps=0, train_batch_size=2, eval_batch_size=2, test_batch_size=2, seed=42, dataloader_drop_last=True, dataloader_num_workers=0, dataloader_pin_memory=True, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None)\n",
            "ModelArguments(model_name_or_path='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', model_type='qwen', config_overrides=None, config_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', tokenizer_name='../data/nlp/pre_models/torch/qwen/Qwen-1_8B', processer_name=None, imageprocesser_name=None, feature_extractor_name=None, cache_dir=None, do_lower_case=None, use_fast_tokenizer=False, model_revision='main', use_auth_token=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, model_custom={})\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:deep_training.nlp.models.qwen.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.20s/it]\n",
            "****************************** total 500\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /content/qwen-finetune-demo/qwen_finetuning-dev/scripts/best_ckpt exists and is not empty.\n",
            "INFO:lightning_fabric.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name             | Type             | Params\n",
            "------------------------------------------------------\n",
            "0 | transformer_base | TransformerForLM | 1.8 B \n",
            "------------------------------------------------------\n",
            "1.8 B     Trainable params\n",
            "0         Non-trainable params\n",
            "1.8 B     Total params\n",
            "7,347.315 Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name             | Type             | Params\n",
            "------------------------------------------------------\n",
            "0 | transformer_base | TransformerForLM | 1.8 B \n",
            "------------------------------------------------------\n",
            "1.8 B     Trainable params\n",
            "0         Non-trainable params\n",
            "1.8 B     Total params\n",
            "7,347.315 Total estimated model params size (MB)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "Epoch 0:   0% 0/250 [00:00<?, ?it/s] Traceback (most recent call last):\n",
            "  File \"/content/qwen-finetune-demo/qwen_finetuning-dev/scripts/../train.py\", line 29, in <module>\n",
            "    main()\n",
            "  File \"/content/qwen-finetune-demo/qwen_finetuning-dev/scripts/../train.py\", line 21, in main\n",
            "    main_execute()\n",
            "  File \"/content/qwen-finetune-demo/qwen_finetuning-dev/training/train_pl.py\", line 154, in main\n",
            "    trainer.fit(pl_model, train_dataloaders=train_datasets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
            "    self.advance(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
            "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
            "    self._optimizer_step(batch_idx, closure)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
            "    call._call_lightning_module_hook(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
            "    output = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
            "    optimizer.step(closure=optimizer_closure)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
            "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
            "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/amp.py\", line 86, in optimizer_step\n",
            "    self.scaler.unscale_(optimizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 307, in unscale_\n",
            "    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 229, in _unscale_grads_\n",
            "    raise ValueError(\"Attempting to unscale FP16 gradients.\")\n",
            "ValueError: Attempting to unscale FP16 gradients.\n",
            "Epoch 0:   0%|          | 0/250 [00:08<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %cd /content/qwen-finetune-demo/qwen_finetuning-dev/infer/\n",
        " !python infer_lora_finetuning.py"
      ],
      "metadata": {
        "id": "CAo3L8zWr6MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hvc7g5iMRZ82"
      }
    }
  ]
}