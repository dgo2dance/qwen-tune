{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/GGUF_Converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "python3 --version\n",
        "make --version\n",
        "g++ --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysh3Cx9hSGY6",
        "outputId": "41b66b91-7aaf-4b04-ef6a-456ff4b5d2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "GNU Make 4.3\n",
            "Built for x86_64-pc-linux-gnu\n",
            "Copyright (C) 1988-2020 Free Software Foundation, Inc.\n",
            "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
            "This is free software: you are free to change and redistribute it.\n",
            "There is NO WARRANTY, to the extent permitted by law.\n",
            "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb003dHqcQdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c46f306-3b0e-4dd4-ebf7-c41f7837b9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 14632, done.\u001b[K\n",
            "remote: Counting objects: 100% (5183/5183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (312/312), done.\u001b[K\n",
            "remote: Total 14632 (delta 5033), reused 4894 (delta 4870), pack-reused 9449\u001b[K\n",
            "Receiving objects: 100% (14632/14632), 16.41 MiB | 16.19 MiB/s, done.\n",
            "Resolving deltas: 100% (10236/10236), done.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy==1.24.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r requirements.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf>=4.21.0 (from -r requirements.txt (line 5))\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0->-r requirements.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.34.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.34.0->-r requirements.txt (line 3)) (2023.11.17)\n",
            "Installing collected packages: sentencepiece, protobuf, numpy, gguf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 protobuf-4.25.1 sentencepiece-0.1.98\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp && make && pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ENwsZ0_fYMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9569d10-86b2-44d0-cd20-023489296beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-transfer\n",
            "  Downloading hf_transfer-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-transfer\n",
            "Successfully installed hf-transfer-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install hf-transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66mMgKSfbbw",
        "outputId": "f51cf5e0-d48a-4f81-d235-34bd35f5a996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/.gitattributes to /root/.cache/huggingface/hub/tmpiyvj13ao\n",
            "\r.gitattributes:   0% 0.00/1.52k [00:00<?, ?B/s]\r.gitattributes: 100% 1.52k/1.52k [00:00<00:00, 3.73MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/README.md to /root/.cache/huggingface/hub/tmp3q00pxti\n",
            "\rREADME.md:   0% 0.00/8.73k [00:00<?, ?B/s]\rREADME.md: 100% 8.73k/8.73k [00:00<00:00, 24.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/Research%20License.docx to /root/.cache/huggingface/hub/tmppc3ttgr8\n",
            "Research License.docx: 100% 38.9k/38.9k [00:00<00:00, 29.6MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/added_tokens.json to /root/.cache/huggingface/hub/tmp04i_95c4\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 4.41MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/config.json to /root/.cache/huggingface/hub/tmpdixevjjl\n",
            "config.json: 100% 732/732 [00:00<00:00, 2.76MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/configuration_phi.py to /root/.cache/huggingface/hub/tmpalphsppd\n",
            "configuration_phi.py: 100% 2.03k/2.03k [00:00<00:00, 6.01MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/generation_config.json to /root/.cache/huggingface/hub/tmp2oajp464\n",
            "generation_config.json: 100% 69.0/69.0 [00:00<00:00, 245kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/merges.txt to /root/.cache/huggingface/hub/tmpodezm9of\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 20.2MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/modeling_phi.py to /root/.cache/huggingface/hub/tmp1aus7yci\n",
            "modeling_phi.py: 100% 33.5k/33.5k [00:00<00:00, 63.7MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/pytorch_model.bin to /root/.cache/huggingface/hub/tmpimgts1rq\n",
            "pytorch_model.bin: 100% 2.84G/2.84G [00:20<00:00, 136MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/special_tokens_map.json to /root/.cache/huggingface/hub/tmp_cic1_yz\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 405kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer.json to /root/.cache/huggingface/hub/tmpr1dlp5fq\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 27.8MB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/tokenizer_config.json to /root/.cache/huggingface/hub/tmpvazjcj8u\n",
            "tokenizer_config.json: 100% 237/237 [00:00<00:00, 993kB/s]\n",
            "downloading https://huggingface.co/microsoft/phi-1_5/resolve/24f9ea14df973a49a0d87c16d04df88d90067468/vocab.json to /root/.cache/huggingface/hub/tmpi0gu_jjc\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 50.4MB/s]\n",
            "/content/phi-1_5\n"
          ]
        }
      ],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli download microsoft/phi-1_5 --local-dir phi-1_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drRiT7wPhgA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738395c1-c037-4cd6-e507-e193e9754661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: phi-1_5\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "Set model parameters\n",
            "Set model tokenizer\n",
            "gguf: Adding 50000 merge(s).\n",
            "gguf: Setting special token type bos to 50256\n",
            "gguf: Setting special token type eos to 50256\n",
            "gguf: Setting special token type unk to 50256\n",
            "Exporting model to 'phi-1_5/ggml-model-f16.gguf'\n",
            "gguf: loading model part 'pytorch_model.bin'\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "token_embd.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.0.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.0.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.1.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.1.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.2.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.2.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.3.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.3.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.4.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.4.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.5.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.5.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.6.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.6.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.7.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.7.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.8.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.8.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.9.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.9.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.10.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.10.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.11.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.11.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.12.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.12.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.13.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.13.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.14.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.14.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.15.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.15.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.16.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.16.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.17.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.17.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.18.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.18.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.19.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.19.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.20.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.20.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.21.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.21.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.22.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.22.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_qkv.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_qkv.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.attn_output.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.attn_output.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_up.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_up.bias, n_dims = 1, torch.float16 --> float32\n",
            "blk.23.ffn_down.weight, n_dims = 2, torch.float16 --> float16\n",
            "blk.23.ffn_down.bias, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.weight, n_dims = 1, torch.float16 --> float32\n",
            "output_norm.bias, n_dims = 1, torch.float16 --> float32\n",
            "output.weight, n_dims = 2, torch.float16 --> float16\n",
            "output.bias, n_dims = 1, torch.float16 --> float32\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 1173, in <module>\n",
            "    model_instance.write()\n",
            "  File \"/content/llama.cpp/convert-hf-to-gguf.py\", line 139, in write\n",
            "    self.gguf_writer.write_tensors_to_file()\n",
            "  File \"/content/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 264, in write_tensors_to_file\n",
            "    tensor.tofile(self.fout)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 llama.cpp/convert-hf-to-gguf.py ./phi-1_5/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd llama.cpp\n",
        "git log | head -1\n",
        "python3 --version\n",
        "pip list | egrep \"torch|numpy|sentencepiece\"\n",
        "make --version | head -1\n",
        "\n",
        "md5sum ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
        "md5sum ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvv4fmVkUEC0",
        "outputId": "94c6b865-df9d-4ca6-a253-706359a8f891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commit 708e179e8562c2604240df95a2241dea17fd808b\n",
            "Python 3.10.12\n",
            "numpy                            1.24.4\n",
            "sentencepiece                    0.1.98\n",
            "torch                            2.1.0+cu121\n",
            "torchaudio                       2.1.0+cu121\n",
            "torchdata                        0.7.0\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.16.0\n",
            "torchvision                      0.16.0+cu121\n",
            "GNU Make 4.3\n",
            "83af84a363c8593a8ed930b4033c4868  ../dolphin-2_6-phi-2/model-00001-of-00002.safetensors\n",
            "706ead5763dbc23b9a1e1388ce7721ee  ../dolphin-2_6-phi-2/model-00002-of-00002.safetensors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMeEnRZpcfoa"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q8_0\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q6_K\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q5_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q4_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q3_K_M\n",
        "!llama.cpp/quantize ./dolphin-2_6-phi-2/ggml-model-f16.gguf Q2_K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd dolphin-2_6-phi-2 && rename 's/^ggml-model-/dolphin-2_6-phi-2-/' ggml-model-*"
      ],
      "metadata": {
        "id": "vsKWnTLw5FV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=true huggingface-cli upload dolphin-2_6-phi-2-GGUF ./dolphin-2_6-phi-2/ ."
      ],
      "metadata": {
        "id": "JEIYZV9Z5OC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX-sZuxxrkYV"
      },
      "outputs": [],
      "source": [
        "!llama.cpp/main -m ./dolphin-2_6-phi-2/ggml-model-Q4_K_M.gguf -p \"What's up?\" -n 400 -e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}