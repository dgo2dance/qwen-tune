{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoj2PkJbT5j9yXS3Wv8Gws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgo2dance/qwen-tune/blob/main/kuaishou_agent_fastapi_llamacpp_python_ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdJeKM2_c3lF"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-50AbTRLCh5D",
        "outputId": "04a27282-b46e-4f6c-bdcc-d7e63691cf62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Git LFS initialized.\n",
            "Cloning into 'kagentlms_qwen_7b_mat_gguf'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 11 (delta 1), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (11/11), 2.58 KiB | 880.00 KiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tggml-model-q4_0.gguf\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"下载代码\n",
        "\"\"\"\n",
        "%cd /content/\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/kwaikeg/kagentlms_qwen_7b_mat_gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_cpp_python[server]"
      ],
      "metadata": {
        "id": "0684LhvlfEbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install llama_cpp_python[server]"
      ],
      "metadata": {
        "id": "nEXjxYE-RyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyngrok\n",
        "!pip install jiwer\n",
        "!pip install tabulate\n",
        "!pip install pydub\n",
        "!pip -q install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio\n",
        "!pip -q install python-multipart"
      ],
      "metadata": {
        "id": "dku2_KtxUSKd",
        "outputId": "ab2d6972-3956-4d3c-c6ba-8933ad5b1392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "\n",
        "import shutil\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "sNF7cLubWBzX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import models for serialisation/ deserialisation\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "import io\n",
        "import wave\n",
        "\n",
        "\n",
        "class AudioBytes(BaseModel):\n",
        "  data: bytes\n",
        "  filename: str\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "\n",
        "@app.get(\"/stt-test\")\n",
        "async def stt_test():\n",
        "  return {\"transcript\":\"Hello from STT.\"}"
      ],
      "metadata": {
        "id": "PWJAT-9ZWDAQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2YvFOacMTFmP0jONlJAi7nUkbjr_48znoXDys5dh7qQDUHYyJ # place_your_ngrok_auth_token_here"
      ],
      "metadata": {
        "id": "SDdGAxZnT7Vj",
        "outputId": "dc25f546-aa66-43d8-fe7f-efdee50895fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create public URL to access Speech-To-Text service\n",
        "from pyngrok import ngrok\n",
        "ngrok_tunnel = ngrok.connect(8090)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "# uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "oJKua9eXT9Ag",
        "outputId": "ed418671-66ba-4098-e47a-1771190e1305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://64fe-34-69-232-70.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "uvicorn.run(app, port=8090)"
      ],
      "metadata": {
        "id": "KGNnn5BgWORw",
        "outputId": "68205b60-2399-4d45-e20f-f002f33537f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [198]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8090 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"GET /stt-test HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m llama_cpp.server --model /content/kagentlms_qwen_7b_mat_gguf/ggml-model-q4_0.gguf  --chat_format chatml --host 127.0.0.1 --port 8090"
      ],
      "metadata": {
        "id": "UZouOsqRSPUP",
        "outputId": "c20f41fe-9406-43b6-e58d-386a393e215e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 259 tensors from /content/kagentlms_qwen_7b_mat_gguf/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
            "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
            "llama_model_loader: - kv   2:                        qwen.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                           qwen.block_count u32              = 32\n",
            "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 22016\n",
            "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                          general.file_type u32              = 2\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type q4_0:  161 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 22016\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 7.72 B\n",
            "llm_load_print_meta: model size       = 4.20 GiB (4.67 BPW) \n",
            "llm_load_print_meta: general.name     = Qwen\n",
            "llm_load_print_meta: BOS token        = 151643 '[PAD151643]'\n",
            "llm_load_print_meta: EOS token        = 151643 '[PAD151643]'\n",
            "llm_load_print_meta: UNK token        = 151643 '[PAD151643]'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4297.21 MiB\n",
            "warning: failed to mlock 356007936-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
            "...................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "llama_new_context_with_model:        CPU compute buffer size =   304.75 MiB\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'general.file_type': '2', 'tokenizer.ggml.unknown_token_id': '151643', 'general.architecture': 'qwen', 'qwen.rope.dimension_count': '128', 'qwen.rope.freq_base': '10000.000000', 'qwen.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'Qwen', 'qwen.context_length': '8192', 'qwen.block_count': '32', 'qwen.attention.head_count': '32', 'qwen.embedding_length': '4096', 'qwen.feed_forward_length': '22016', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151643'}\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8973\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8090\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\n",
            "llama_print_timings:        load time =   35398.13 ms\n",
            "llama_print_timings:      sample time =     187.07 ms /    66 runs   (    2.83 ms per token,   352.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35397.91 ms /    48 tokens (  737.46 ms per token,     1.36 tokens per second)\n",
            "llama_print_timings:        eval time =   59990.37 ms /    65 runs   (  922.93 ms per token,     1.08 tokens per second)\n",
            "llama_print_timings:       total time =   97603.96 ms /   113 tokens\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   35398.13 ms\n",
            "llama_print_timings:      sample time =     381.62 ms /   130 runs   (    2.94 ms per token,   340.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15900.61 ms /    22 tokens (  722.76 ms per token,     1.38 tokens per second)\n",
            "llama_print_timings:        eval time =  125606.06 ms /   129 runs   (  973.69 ms per token,     1.03 tokens per second)\n",
            "llama_print_timings:       total time =  144209.31 ms /   151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   35398.13 ms\n",
            "llama_print_timings:      sample time =     188.30 ms /    68 runs   (    2.77 ms per token,   361.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =   58643.77 ms /    68 runs   (  862.41 ms per token,     1.16 tokens per second)\n",
            "llama_print_timings:       total time =   59825.98 ms /    69 tokens\n",
            "\u001b[32mINFO\u001b[0m:     2400:8901:e001:2e7:e164:7c11:a003:bccf:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m8973\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 747, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "  File \"/usr/lib/python3.10/asyncio/queues.py\", line 159, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('查看内存容量')\n",
        "!cat /proc/meminfo | grep MemTotal\n",
        ""
      ],
      "metadata": {
        "id": "NIMZfyBOT2iA",
        "outputId": "17e882c0-41a7-435a-82d6-c73b2acb1bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查看内存容量\n",
            "MemTotal:       13290480 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! lsof -i -n"
      ],
      "metadata": {
        "id": "zWJqStq2TUJL",
        "outputId": "07cc1429-72a0-4685-cc80-c7a4b584fc26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "node        6 root   21u  IPv6  18762      0t0  TCP *:8080 (LISTEN)\n",
            "node        6 root   27u  IPv4  48461      0t0  TCP 172.28.0.12:41950->172.28.0.12:6000 (ESTABLISHED)\n",
            "node        6 root   28u  IPv6 107237      0t0  TCP 172.28.0.12:8080->172.28.0.1:54314 (ESTABLISHED)\n",
            "node        6 root   29u  IPv6 103733      0t0  TCP 172.28.0.12:8080->172.28.0.1:47402 (ESTABLISHED)\n",
            "node        6 root   34u  IPv6 104794      0t0  TCP 172.28.0.12:8080->172.28.0.1:46602 (ESTABLISHED)\n",
            "node        6 root   36u  IPv6 106845      0t0  TCP 172.28.0.12:8080->172.28.0.1:51054 (ESTABLISHED)\n",
            "kernel_ma  21 root    3u  IPv4  18268      0t0  TCP 172.28.0.12:6000 (LISTEN)\n",
            "kernel_ma  21 root    7u  IPv4  48111      0t0  TCP 172.28.0.12:6000->172.28.0.12:41950 (ESTABLISHED)\n",
            "kernel_ma  21 root    8u  IPv4  19745      0t0  TCP 172.28.0.12:36772->172.28.0.12:9000 (ESTABLISHED)\n",
            "kernel_ma  21 root   10u  IPv4  23334      0t0  TCP 172.28.0.12:6000->172.28.0.1:48564 (ESTABLISHED)\n",
            "kernel_ma  21 root   11u  IPv4  48115      0t0  TCP 172.28.0.12:54498->172.28.0.12:9000 (ESTABLISHED)\n",
            "colab-fil  63 root    3u  IPv4  18797      0t0  TCP 127.0.0.1:3453 (LISTEN)\n",
            "jupyter-n  85 root    7u  IPv4  19110      0t0  TCP 172.28.0.12:9000 (LISTEN)\n",
            "jupyter-n  85 root    8u  IPv4  19157      0t0  TCP 172.28.0.12:9000->172.28.0.12:36772 (ESTABLISHED)\n",
            "jupyter-n  85 root   27u  IPv4  48116      0t0  TCP 172.28.0.12:9000->172.28.0.12:54498 (ESTABLISHED)\n",
            "dap_multi  86 root    9u  IPv4  22575      0t0  TCP 127.0.0.1:56792->127.0.0.1:33639 (ESTABLISHED)\n",
            "python3   198 root   21u  IPv4  20942      0t0  TCP 127.0.0.1:43275 (LISTEN)\n",
            "python3   198 root   40u  IPv4  21350      0t0  TCP 127.0.0.1:48754->127.0.0.1:55101 (ESTABLISHED)\n",
            "python3   245 root    3u  IPv4  21344      0t0  TCP 127.0.0.1:33639 (LISTEN)\n",
            "python3   245 root    4u  IPv4  21345      0t0  TCP 127.0.0.1:55101 (LISTEN)\n",
            "python3   245 root    5u  IPv4  21346      0t0  TCP 127.0.0.1:33639->127.0.0.1:56792 (ESTABLISHED)\n",
            "python3   245 root    6u  IPv4  21348      0t0  TCP 127.0.0.1:55101->127.0.0.1:48754 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hvc7g5iMRZ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "fhUvLXgjHm_Z",
        "outputId": "cab724b9-c293-4ac9-d657-67bff888622f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/kagentlms_qwen_7b_mat_gguf/ggml-model-q4_0.gguf   \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "id": "YEqi2aTnJT64"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}